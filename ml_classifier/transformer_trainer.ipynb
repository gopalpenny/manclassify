{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7217f6c2-4c4c-4e5c-9bb6-8687ed8f5cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import re\n",
    "import sys\n",
    "from datetime import timedelta\n",
    "# from torch.nn.functional import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f95fcad-56ab-43b6-a50d-317152f7e186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loc_id</th>\n",
       "      <th>Subclass2019</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Plantation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Crop(Single)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Crop(Single)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Crop(Single)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Plantation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>496</td>\n",
       "      <td>Crop(Single)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>497</td>\n",
       "      <td>Crop(Single)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>498</td>\n",
       "      <td>Plantation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>499</td>\n",
       "      <td>Plantation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>500</td>\n",
       "      <td>Crop(Double)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>501 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     loc_id  Subclass2019\n",
       "0         0    Plantation\n",
       "1         1  Crop(Single)\n",
       "2         2  Crop(Single)\n",
       "3         3  Crop(Single)\n",
       "4         4    Plantation\n",
       "..      ...           ...\n",
       "496     496  Crop(Single)\n",
       "497     497  Crop(Single)\n",
       "498     498    Plantation\n",
       "499     499    Plantation\n",
       "500     500  Crop(Double)\n",
       "\n",
       "[501 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "proj_paths = [\"/Users/gopal/Google Drive/_Research/Research projects/ML/manclassify/app_data/Thailand\",\n",
    "              \"/Users/gopalpenny/Library/CloudStorage/GoogleDrive-gopalpenny@gmail.com/My Drive/_Research/Research projects/ML/manclassify/app_data/Thailand\"]\n",
    "\n",
    "proj_path = [path for path in proj_paths if os.path.exists(path)][0]\n",
    "\n",
    "class_path = os.path.join(proj_path,\"Thailand_classification\")\n",
    "ts_path = os.path.join(proj_path,\"Thailand_download_timeseries\")\n",
    "# pd.read_csv(\"\n",
    "os.listdir(class_path)\n",
    "\n",
    "loc_id = 0\n",
    "\n",
    "s2_csv_name = f\"pt_ts_loc{loc_id}_s2.csv\"\n",
    "s2_csv_name\n",
    "\n",
    "class_colname = 'Subclass2019'\n",
    "\n",
    "proj_normpath = os.path.normpath(proj_path)\n",
    "proj_dirname = proj_normpath.split(os.sep)[-1]\n",
    "proj_name = re.sub(\"_classification$\",\"\",proj_dirname)\n",
    "class_path = os.path.join(proj_path, proj_name + \"_classification\")\n",
    "ts_path = os.path.join(proj_path, proj_name + \"_download_timeseries\")\n",
    "pt_classes = pd.read_csv(os.path.join(class_path,\"location_classification.csv\"))\n",
    "pt_classes = pt_classes[['loc_id', class_colname]].dropna()\n",
    "\n",
    "pt_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadc6355-df65-4834-b219-1d3d6ead823f",
   "metadata": {},
   "source": [
    "## Generate the torch tensor dataset\n",
    "\n",
    "### Define function to read timeseries\n",
    "\n",
    "* Read timeseries\n",
    "* Filter timeseries to date range (+/- 60 days)\n",
    "* Remove observations with clouds\n",
    "* Take the mean value for each day (occurs when multiple overpasses happen on the same day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b32be994-4f4d-4666-9e96-9424cc71f3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep dataset\n",
    "date_range = pd.to_datetime(['2019-06-01','2020-05-31'])\n",
    "\n",
    "def prep_s2_loc(loc_id, date_range, proj_path):\n",
    "    ts_path = os.path.join(proj_path,\"Thailand_download_timeseries\")\n",
    "    s2_csv_name = f\"pt_ts_loc{loc_id}_s2.csv\"\n",
    "    s2_csv_path = os.path.join(ts_path, s2_csv_name)\n",
    "    s2_ts = pd.read_csv(s2_csv_path)\n",
    "\n",
    "    # extract dates from image ids\n",
    "    s2_ts['datestr'] = [re.sub(\"(^[0-9]+)[a-zA-Z].*\",\"\\\\1\",x) for x in s2_ts.image_id]\n",
    "    s2_ts['date'] = pd.to_datetime(s2_ts.datestr, format = \"%Y%m%d\")\n",
    "\n",
    "    # subset to cloud-free days AND within date_range\n",
    "    s2_ts = s2_ts[(s2_ts.date >= date_range[0] - timedelta(days = 60)) & \n",
    "                  (s2_ts.date <= date_range[1] + timedelta(days = 60)) & \n",
    "                  (s2_ts.cloudmask == 0)]\n",
    "\n",
    "    # calculate day from startday\n",
    "    date_diff = (s2_ts.date - date_range[0])\n",
    "    s2_ts['day'] = [x.days for x in date_diff]\n",
    "    s2_ts['loc_id'] = loc_id\n",
    "\n",
    "    # select only predictor and position columns, return tensor\n",
    "    s2_ts_x = s2_ts[['loc_id','day','B8','B4','B3','B2']]\n",
    "    return s2_ts_x\n",
    "\n",
    "# s2_ts_loc125 = prep_s2_loc(125, date_range, proj_path)\n",
    "# s2_ts_loc125.groupby(['loc_id','day'],as_index = False).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6336b48f-e6ff-468b-95cf-a1396e0e1768",
   "metadata": {},
   "source": [
    "### Get the torch tensor dataset (prep and save OR read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38ac523f-49d7-4427-8d02-85e905ab73f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from ipywidgets import IntProgress\n",
    "# from IPython.display import display\n",
    "\n",
    "if os.path.exists(os.path.join(proj_path, 's2_ts_prepped.pt')):\n",
    "    loc_ts_tor = torch.load(os.path.join(proj_path, 's2_ts_prepped.pt'))\n",
    "    \n",
    "else:\n",
    "    # f = IntProgress(min=0, max=pt_classes.shape[0]) # instantiate the bar\n",
    "    display(f) # display the bar\n",
    "    \n",
    "    s2_ts_list = []\n",
    "    loc_id_list = []\n",
    "    for i in np.arange(pt_classes.shape[0]):\n",
    "        # loc_id = 499\n",
    "        # print(loc_id)\n",
    "        loc_id = pt_classes.loc_id.iloc[i]\n",
    "        # loc_id_list.append(loc_id)\n",
    "        s2_ts_loc = prep_s2_loc(loc_id, date_range, proj_path)\n",
    "        s2_ts_loc = s2_ts_loc.groupby(['loc_id','day'],as_index = False).mean()\n",
    "        s2_ts_tor = torch.tensor(s2_ts_loc.to_numpy())\n",
    "        s2_ts_list.append(s2_ts_tor)\n",
    "        # f.value += 1\n",
    "        \n",
    "    loc_ts_tor = torch.cat(s2_ts_list)\n",
    "\n",
    "    torch.save(loc_ts_tor, os.path.join(proj_path, 's2_ts_prepped.pt'))\n",
    "\n",
    "sys.getsizeof(loc_ts_tor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87657506-61f2-446c-ab16-6d6a4b200b16",
   "metadata": {},
   "source": [
    "### Prep the dataset tensors\n",
    "\n",
    "* Subset to training classes (crops & plantations)\n",
    "* Check max number of rows\n",
    "* Normalize & center\n",
    "* Split loc_id into training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7b06a55-3c55-42e7-91fa-6ca3ea412559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All classes\n",
      "              loc_id\n",
      "Subclass2019        \n",
      "Crop(Double)      68\n",
      "Crop(Single)     278\n",
      "Forest             3\n",
      "Golf               1\n",
      "Mixed             20\n",
      "Plantation       109\n",
      "Unsure            17\n",
      "Urban              1\n",
      "Water              4\n",
      "\n",
      "Training dataset (pt_classes_ag)\n",
      "      loc_id  Subclass2019\n",
      "0         0    Plantation\n",
      "1         1  Crop(Single)\n",
      "2         2  Crop(Single)\n",
      "3         3  Crop(Single)\n",
      "4         4    Plantation\n",
      "..      ...           ...\n",
      "496     496  Crop(Single)\n",
      "497     497  Crop(Single)\n",
      "498     498    Plantation\n",
      "499     499    Plantation\n",
      "500     500  Crop(Double)\n",
      "\n",
      "[455 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print('All classes')\n",
    "print(pt_classes.groupby('Subclass2019').count())\n",
    "\n",
    "train_classes = ['Crop(Double)','Crop(Single)','Plantation']\n",
    "pt_classes_ag = pt_classes[pt_classes['Subclass2019'].isin(train_classes)]\n",
    "print('\\nTraining dataset (pt_classes_ag)\\n',pt_classes_ag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2a727f0-a8c6-415c-b482-e19f70c5f571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of observations for any loc_id\n",
      "     loc_id  num_obs\n",
      "481     481       94\n"
     ]
    }
   ],
   "source": [
    "loc_ts_tor = loc_ts_tor[(loc_ts_tor[:,1] >= -30) & (loc_ts_tor[:,1] <= 395)]\n",
    "\n",
    "row_means= loc_ts_tor.mean(dim = 1)#.shape #.unsqueeze(0).repeat(5,1)\n",
    "loc_ts_tor = loc_ts_tor[~torch.isnan(row_means)]\n",
    "col_means= loc_ts_tor.mean(dim = 0)#.shape #.unsqueeze(0).repeat(5,1)\n",
    "col_std= loc_ts_tor.std(dim = 0)#.shape #.unsqueeze(0).repeat(5,1)\n",
    "col_means[[0,1]] = 0\n",
    "col_std[[0,1]] = 1\n",
    "\n",
    "loc_ts_tor_std = col_std.unsqueeze(0).repeat(loc_ts_tor.shape[0],1)\n",
    "loc_ts_tor_mean = col_means.unsqueeze(0).repeat(loc_ts_tor.shape[0],1)\n",
    "\n",
    "loc_ts_norm = (loc_ts_tor - loc_ts_tor_mean) / loc_ts_tor_std\n",
    "\n",
    "# get max of number of observations per location\n",
    "# idx = np.arange(loc_ts_norm.shape[0])\n",
    "loc_id = np.unique(loc_ts_norm[:,0])\n",
    "num_obs = pd.DataFrame({'loc_id' : np.unique(loc_ts_norm[:,0]).astype('int')})\n",
    "num_obs['num_obs'] = [loc_ts_norm[loc_ts_norm[:,0]==i,:].shape[0] for i in num_obs['loc_id']]\n",
    "print(\"Max number of observations for any loc_id\")\n",
    "print(num_obs.iloc[[num_obs['num_obs'].idxmax()]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd6b44a2-91e1-4bfd-8b55-cdba3b3f1f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (loc_train summary)\n",
      "               loc_id    n  weight\n",
      "Subclass2019                     \n",
      "Crop(Double)      54   54      54\n",
      "Crop(Single)     222  222     222\n",
      "Plantation        87   87      87\n",
      "\n",
      "Testing (loc_test summary)\n",
      "               loc_id\n",
      "Subclass2019        \n",
      "Crop(Double)      14\n",
      "Crop(Single)      56\n",
      "Plantation        22\n"
     ]
    }
   ],
   "source": [
    "loc_train = pt_classes_ag.groupby('Subclass2019', group_keys = False).apply(lambda x: x.sample(frac = 0.8))\n",
    "loc_train['n'] = loc_train.groupby('Subclass2019')['loc_id'].transform(len)\n",
    "loc_train['weight'] = loc_train.shape[0] / loc_train['n'] \n",
    "\n",
    "loc_test = pt_classes_ag[~pt_classes_ag['loc_id'].isin(loc_train.loc_id)]\n",
    "print('Training (loc_train summary)\\n', loc_train.groupby('Subclass2019').count())\n",
    "print('\\nTesting (loc_test summary)\\n', loc_test.groupby('Subclass2019').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bdda10b-384d-45fa-96d5-e83ec73a93f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loc_id</th>\n",
       "      <th>Subclass2019</th>\n",
       "      <th>n</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>166</td>\n",
       "      <td>Crop(Double)</td>\n",
       "      <td>54</td>\n",
       "      <td>6.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>279</td>\n",
       "      <td>Crop(Double)</td>\n",
       "      <td>54</td>\n",
       "      <td>6.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>172</td>\n",
       "      <td>Crop(Double)</td>\n",
       "      <td>54</td>\n",
       "      <td>6.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>65</td>\n",
       "      <td>Crop(Double)</td>\n",
       "      <td>54</td>\n",
       "      <td>6.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>445</td>\n",
       "      <td>Crop(Double)</td>\n",
       "      <td>54</td>\n",
       "      <td>6.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>124</td>\n",
       "      <td>Plantation</td>\n",
       "      <td>87</td>\n",
       "      <td>4.172414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>221</td>\n",
       "      <td>Plantation</td>\n",
       "      <td>87</td>\n",
       "      <td>4.172414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>78</td>\n",
       "      <td>Plantation</td>\n",
       "      <td>87</td>\n",
       "      <td>4.172414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>454</td>\n",
       "      <td>Plantation</td>\n",
       "      <td>87</td>\n",
       "      <td>4.172414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>120</td>\n",
       "      <td>Plantation</td>\n",
       "      <td>87</td>\n",
       "      <td>4.172414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>363 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     loc_id  Subclass2019   n    weight\n",
       "166     166  Crop(Double)  54  6.722222\n",
       "279     279  Crop(Double)  54  6.722222\n",
       "172     172  Crop(Double)  54  6.722222\n",
       "65       65  Crop(Double)  54  6.722222\n",
       "445     445  Crop(Double)  54  6.722222\n",
       "..      ...           ...  ..       ...\n",
       "124     124    Plantation  87  4.172414\n",
       "221     221    Plantation  87  4.172414\n",
       "78       78    Plantation  87  4.172414\n",
       "454     454    Plantation  87  4.172414\n",
       "120     120    Plantation  87  4.172414\n",
       "\n",
       "[363 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "262ac0dc-59b7-4231-8c0d-c1f0062b069a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo = np.random.rand(4,3)\n",
    "\n",
    "np.sum(foo[:,0] > 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b147e879-0ce6-49b4-a45e-b41e3cb0dca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loc_ids_all = np.unique(x_train[:,0])\n",
    "# loc_ids_all\n",
    "\n",
    "# np.unique(loc_train['n'])\n",
    "# i = loc_ids_all[0]\n",
    "# [i for i in loc_ids_all]\n",
    "# torch.tensor.to_numpy(x_train[:,0] == i)\n",
    "# np.sum(x_train[:,0] == i)\n",
    "# [np.sum(x_train[:,0] == i) for i in np.unique(x_train[:,0])]\n",
    "# for i in loc_ids_all:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "848fb5a1-70fc-4de4-8d58-ebaea6adaa25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class s2Dataset(Dataset):\n",
    "    \"\"\"Sentinel 2 dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, x_train, y_train, max_obs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_train (tensor): contains loc_id and predictors as columns, s2 observations as rows\n",
    "            y_train (tensor): contains loc_id as rows, weights and class as 1-hot columns\n",
    "        \"\"\"\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.max_obs = max_obs\n",
    "        # self.proj_path = proj_path\n",
    "        # proj_normpath = os.path.normpath(proj_path)\n",
    "        # proj_dirname = proj_normpath.split(os.sep)[-1]\n",
    "        # self.proj_name = re.sub(\"_classification$\",\"\",proj_dirname)\n",
    "        # self.class_path = os.path.join(proj_path, self.proj_name + \"_classification\")\n",
    "        # self.ts_path = os.path.join(proj_path, self.proj_name + \"_download_timeseries\")\n",
    "        # self.pt_classes = pd.read_csv(os.path.join(self.class_path,\"location_classification.csv\"))\n",
    "        # self.pt_classes = classes[['loc_id', class_colname]].dropna()\n",
    "        # self.classes = pd.unique(self.pt_classes[class_colname])\n",
    "        # self.labels = self.pt_classes.assign(val = 1).pivot_table(columns = class_colname, index = 'loc_id', values = 'val', fill_value= 0)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # get loc_id\n",
    "        loc_id = self.y_train[idx,0]\n",
    "        self.last_loc_id = loc_id\n",
    "        \n",
    "        # select location id\n",
    "        x_loc = self.x_train[self.x_train[:,0]==loc_id]\n",
    "        x_prep = x_loc[:,1:] # remove loc_id column\n",
    "        \n",
    "        # pad zeros to max_obs\n",
    "        n_pad = self.max_obs - x_prep.shape[0]\n",
    "        \n",
    "        x = torch.cat((x_prep, torch.zeros(n_pad, x_prep.shape[1])), dim = 0)\n",
    "        \n",
    "        x = x.float()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # get one-hot encoding for the point as tensor\n",
    "        y = torch.tensor(y_train[idx,2:])\n",
    "        \n",
    "        return x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "064c8db3-3b4b-49fa-a8c6-a4862ceff953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1117, 0.8158, 0.2626, 0.1117, 0.8158, 0.2626],\n",
       "        [0.4839, 0.6765, 0.7539, 0.4839, 0.6765, 0.7539]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.random.manual_seed(100)\n",
    "a = torch.rand(2,3)\n",
    "torch.cat((a,a), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90680b25-01d0-4c4e-972d-034ea76b5b03",
   "metadata": {
    "tags": []
   },
   "source": [
    "### get training data\n",
    "\n",
    "* `y_train` directly from `loc_train` & pivot\n",
    "* `x_train` from `loc_ts_norm`, subset to `y_train[:,0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58824da7-827c-430e-9954-0b03a8a80277",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train:\n",
      " [[  0.           4.17241379   0.           0.           1.        ]\n",
      " [  1.           1.63513514   0.           1.           0.        ]\n",
      " [  2.           1.63513514   0.           1.           0.        ]\n",
      " ...\n",
      " [497.           1.63513514   0.           1.           0.        ]\n",
      " [498.           4.17241379   0.           0.           1.        ]\n",
      " [499.           4.17241379   0.           0.           1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# get y_train values from loc_train\n",
    "y_train_df = (loc_train.assign(val = 1) \\\n",
    "  .pivot_table(columns = class_colname, index = ['loc_id','weight'], values = 'val', fill_value= 0) \\\n",
    "  .reset_index(['loc_id','weight']))\n",
    "y_train = y_train_df.to_numpy()\n",
    "print('y_train:\\n',y_train)\n",
    "\n",
    "y_train = y_train_df.to_numpy()\n",
    "\n",
    "# get x_train values from loc_ts_norm (based on loc_id)\n",
    "x_train = loc_ts_norm[torch.isin(loc_ts_norm[:,0],torch.tensor(y_train[:,0]).to(torch.float64)),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c631a3d7-b607-474a-a6ef-6a8751ef1aad",
   "metadata": {},
   "source": [
    "### build pytorch dataset: `s2_dateset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "43ba874b-c3df-4eed-9511-8103df128f75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x example, shape: torch.Size([100, 5]) \n",
      "(idx=2) columns: day, B8, B4, B3, B2\n",
      " tensor([[-1.2000e+01,  1.2615e+00, -2.6081e-01,  3.0071e-01,  5.2696e-02],\n",
      "        [-7.0000e+00, -3.9504e-01,  3.6840e-01,  6.7323e-01,  7.4909e-01],\n",
      "        [ 3.0000e+00,  4.5408e-02,  1.8113e+00,  2.3523e+00,  2.2565e+00],\n",
      "        [ 1.0800e+02, -1.0164e+00, -8.9497e-01, -7.8398e-01, -5.6436e-01],\n",
      "        [ 1.1800e+02, -1.7902e-01, -1.1190e+00, -9.3737e-01, -9.5810e-01],\n",
      "        [ 1.2300e+02, -4.3852e-01, -1.1486e+00, -9.7298e-01, -8.8758e-01],\n",
      "        [ 1.2800e+02,  1.0011e-01, -1.1602e+00, -8.7985e-01, -1.0991e+00],\n",
      "        [ 1.3300e+02,  4.2694e-01, -9.9709e-01, -5.8403e-01, -3.2929e-01],\n",
      "        [ 1.4300e+02,  4.5780e-01, -1.3430e+00, -1.3044e+00, -1.2813e+00],\n",
      "        [ 1.6300e+02,  7.8603e-01, -6.9072e-01, -9.6463e-02, -6.3488e-01],\n",
      "        [ 1.6800e+02,  5.8545e-01,  8.6584e-01,  1.0786e+00,  7.2852e-01],\n",
      "        [ 1.7300e+02, -5.0893e-03,  8.2136e-01,  5.0888e-01,  3.4947e-01],\n",
      "        [ 1.7800e+02, -2.8703e-01,  5.8911e-01,  8.9797e-02, -1.4417e-01],\n",
      "        [ 1.8300e+02, -2.7862e-01,  6.2041e-01,  1.8567e-01,  9.3833e-02],\n",
      "        [ 1.8800e+02, -1.6219e-01,  5.6111e-01,  1.4184e-01, -1.9465e-04],\n",
      "        [ 1.9300e+02, -1.8604e-01,  5.6605e-01,  2.7058e-01,  3.2302e-01],\n",
      "        [ 1.9800e+02, -3.2350e-01,  5.1170e-01, -8.8114e-03, -1.5593e-01],\n",
      "        [ 2.0300e+02, -5.6897e-01,  3.7499e-01, -1.4290e-02,  1.3203e-01],\n",
      "        [ 2.0800e+02, -1.5635e+00, -2.7070e-01, -8.3876e-01, -7.7004e-01],\n",
      "        [ 2.1300e+02, -9.0654e-02,  7.8018e-01,  5.7462e-01,  5.4046e-01],\n",
      "        [ 2.1800e+02, -6.5875e-01,  3.4534e-01, -5.5376e-02, -4.4270e-02],\n",
      "        [ 2.2300e+02, -8.8037e-01,  1.9215e-01, -2.7724e-01, -1.0010e-01],\n",
      "        [ 2.2800e+02, -7.7657e-01,  4.7381e-01,  5.2258e-01,  9.0482e-01],\n",
      "        [ 2.4300e+02, -2.9264e-01,  6.0229e-01,  1.7471e-01,  1.8492e-01],\n",
      "        [ 2.4800e+02, -4.7079e-01,  7.8183e-01,  9.0605e-01,  1.1898e+00],\n",
      "        [ 2.5800e+02, -3.3613e-01,  6.7312e-01,  1.9388e-01,  1.5260e-01],\n",
      "        [ 2.6300e+02, -3.1789e-01,  7.3736e-01,  4.6232e-01,  7.2558e-01],\n",
      "        [ 2.6800e+02, -1.9726e-01,  9.1360e-01,  7.8553e-01,  1.0106e+00],\n",
      "        [ 2.7300e+02, -3.2210e-01,  9.2349e-01,  1.1197e+00,  1.6394e+00],\n",
      "        [ 2.8300e+02, -1.5798e-01,  8.5925e-01,  5.4449e-01,  6.1392e-01],\n",
      "        [ 2.9800e+02, -1.0700e-02,  9.9925e-01,  7.5266e-01,  8.0785e-01],\n",
      "        [ 3.0300e+02,  3.1381e-02,  1.0833e+00,  9.4440e-01,  1.1663e+00],\n",
      "        [ 3.2300e+02, -3.9644e-01,  7.7030e-01,  3.4727e-01,  3.3184e-01],\n",
      "        [ 3.3300e+02,  2.4319e-01,  1.0042e+00,  1.3854e+00,  1.7893e+00],\n",
      "        [ 3.4800e+02,  1.2396e-01,  1.7554e-02,  3.2810e-01,  4.1118e-01],\n",
      "        [ 3.6800e+02,  3.5821e-01, -5.1283e-01, -3.4298e-01, -5.4967e-01],\n",
      "        [ 3.9300e+02, -5.1708e-01, -4.9636e-01, -3.8133e-01, -1.9706e-01],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
      "\n",
      "\n",
      "y example (idx=2): crops(double) crops(single) plantation\n",
      " tensor([0., 1., 0.], dtype=torch.float64)\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "s2_dataset = s2Dataset(x_train = x_train, y_train = y_train, max_obs = 100)\n",
    "\n",
    "# example item in dataset\n",
    "idx_test = 2\n",
    "x, y = s2_dataset.__getitem__(idx_test)\n",
    "\n",
    "print(f'x example, shape: {x.shape} \\n(idx={idx_test}) columns: day, B8, B4, B3, B2\\n',x)\n",
    "# print()\n",
    "print(f'\\n\\ny example (idx={idx_test}): crops(double) crops(single) plantation\\n',y)\n",
    "print(y.shape)\n",
    "# sys.getsizeof(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298322ee-b1f3-42a9-a7c9-de79fb131fdc",
   "metadata": {},
   "source": [
    "### generate sampling weights for data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c911f46-100f-424d-9c1f-c9982d0cd300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loc_id</th>\n",
       "      <th>Subclass2019</th>\n",
       "      <th>n</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>166</td>\n",
       "      <td>Crop(Double)</td>\n",
       "      <td>54</td>\n",
       "      <td>6.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>279</td>\n",
       "      <td>Crop(Double)</td>\n",
       "      <td>54</td>\n",
       "      <td>6.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>172</td>\n",
       "      <td>Crop(Double)</td>\n",
       "      <td>54</td>\n",
       "      <td>6.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>65</td>\n",
       "      <td>Crop(Double)</td>\n",
       "      <td>54</td>\n",
       "      <td>6.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>445</td>\n",
       "      <td>Crop(Double)</td>\n",
       "      <td>54</td>\n",
       "      <td>6.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>124</td>\n",
       "      <td>Plantation</td>\n",
       "      <td>87</td>\n",
       "      <td>4.172414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>221</td>\n",
       "      <td>Plantation</td>\n",
       "      <td>87</td>\n",
       "      <td>4.172414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>78</td>\n",
       "      <td>Plantation</td>\n",
       "      <td>87</td>\n",
       "      <td>4.172414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>454</td>\n",
       "      <td>Plantation</td>\n",
       "      <td>87</td>\n",
       "      <td>4.172414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>120</td>\n",
       "      <td>Plantation</td>\n",
       "      <td>87</td>\n",
       "      <td>4.172414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>363 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     loc_id  Subclass2019   n    weight\n",
       "166     166  Crop(Double)  54  6.722222\n",
       "279     279  Crop(Double)  54  6.722222\n",
       "172     172  Crop(Double)  54  6.722222\n",
       "65       65  Crop(Double)  54  6.722222\n",
       "445     445  Crop(Double)  54  6.722222\n",
       "..      ...           ...  ..       ...\n",
       "124     124    Plantation  87  4.172414\n",
       "221     221    Plantation  87  4.172414\n",
       "78       78    Plantation  87  4.172414\n",
       "454     454    Plantation  87  4.172414\n",
       "120     120    Plantation  87  4.172414\n",
       "\n",
       "[363 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc_train_n = loc_train\n",
    "loc_train_n['n'] = loc_train_n.groupby('Subclass2019')['loc_id'].transform(len)\n",
    "loc_train_n['weight'] = loc_train_n.shape[0] / loc_train_n['n'] \n",
    "loc_train_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e1ad4e3d-3d90-45b7-8fcf-180c28fdcc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(s2_dataset, batch_size = 5, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a2bbd30-e297-47cf-a025-d8983de6c8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0bb5ebc9-1110-4a56-a1e4-6546698d6905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 100, 5])\n",
      "tensor([[-1.3000e+01,  9.9363e-01, -1.2507e+00, -1.1154e+00, -9.3165e-01],\n",
      "        [ 2.0000e+00, -5.6989e-02, -5.6439e-04,  6.8418e-01,  8.0492e-01],\n",
      "        [ 7.0000e+00,  1.6108e+00, -1.2738e+00, -9.8120e-01, -9.2872e-01]])\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(dataloader))\n",
    "tf_test = train_features[:,:,:]\n",
    "# tf_test\n",
    "# train_labels\n",
    "# tf_test\n",
    "tf_test = tf_test.float()\n",
    "print(tf_test.shape)\n",
    "\n",
    "print(tf_test[0, 0:3, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "81301d5a-edc6-4b75-a700-6903c00c522f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3318b320-8a22-4274-8b1f-057fdea17c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PositionalEncoding(nn.Module):\n",
    "\n",
    "#     def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "#         super().__init__()\n",
    "#         self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "#         position = torch.arange(max_len).unsqueeze(1)\n",
    "#         div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "#         pe = torch.zeros(max_len, 1, d_model)\n",
    "#         pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "#         self.register_buffer('pe', pe)\n",
    "\n",
    "#     def forward(self, x: Tensor) -> Tensor:\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "#         \"\"\"\n",
    "#         x = x + self.pe[:x.size(0)]\n",
    "#         return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "620c3037-9bfe-4bca-aafa-a3f086326271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "nhead = 6 # number of attention heads\n",
    "head_dim = 8 # dimension of each word for each attention head\n",
    "dmodel = nhead * head_dim # embed_dim -- each word (row) is embedded to this dimension then split\n",
    "# across the nhead attention heads\n",
    "\n",
    "data_in = tf_test[:, :, 1:] # select only the data\n",
    "positions = tf_test[:,:,0:1] # split out positional data\n",
    "data_dim = data_in.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9864c574-a237-4233-85ed-f4abda964aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5097])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(torch.tensor([5.2333e-01]))/torch.sum(torch.exp(torch.tensor([-1.3249e-01, 5.2333e-01, -2.9124e-01])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "b2cc3947-ba93-4b8c-81a1-75732d2cef06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn, Tensor\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, ntoken: int, dmodel: int, nhead: int, dhid: int, \n",
    "                 nlayers: int, data_dim: int, nclasses: int):\n",
    "        \"\"\"\n",
    "        data_dim: dimension of data (i.e., num of columns) including position as first dimension\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.positional_layer = nn.Linear(1, dmodel)\n",
    "        self.embed_layer = nn.Linear(data_dim - 1, dmodel) # transform data to embed dimension (dmodel)\n",
    "        \n",
    "        # dim_feedforward: https://stackoverflow.com/questions/68087780/pytorch-transformer-argument-dim-feedforward\n",
    "        # shortly: dim_feedforward is a hidden layer between two forward layers at the end of the encoder layer, passed for each word one-by-one\n",
    "        self.encoderlayer = nn.TransformerEncoderLayer(d_model = dmodel, nhead = nhead, dim_feedforward = dhid)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoderlayer, nlayers)\n",
    "        \n",
    "        self.num_params = ntoken * dmodel\n",
    "        \n",
    "        self.class_encoder = nn.Linear(dmodel, nclasses)\n",
    "    \n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        \n",
    "        positions = src[:, :, 0:1]\n",
    "        data = src[:, :, 1:]\n",
    "        pe = self.positional_layer(positions)\n",
    "        data_embed = self.embed_layer(data)\n",
    "        data_and_pe = pe + data_embed\n",
    "        encoder_out = self.encoder(data_and_pe)\n",
    "        \n",
    "        maxpool = torch.max(encoder_out,dim = 1)[0]\n",
    "        \n",
    "        # softmax ensures output of model is probability of class membership -- which sum to 1\n",
    "        # BUT this is already done with CrossEntropyLoss so it's not necessary for this loss function\n",
    "        classes = self.class_encoder(maxpool), dim = 1\n",
    "        \n",
    "        # classes = nn.functional.softmax(classes, 1) # don't use softmax with cross entropy loss... or do?\n",
    "        # don't: https://stackoverflow.com/questions/55675345/should-i-use-softmax-as-output-when-using-cross-entropy-loss-in-pytorch\n",
    "        # do: Machine Learning with Pytorch and Scikitlearn (p 471: Loss functions for classifiers) -- BUT NOT WITH CROSS ENTROPY LOSS (p478\n",
    "        \n",
    "        return classes\n",
    "\n",
    "        # data_in = tf_test[:, :, 1:] # select only the data\n",
    "        # positions = tf_test[:,:,0:1] # split out positional data\n",
    "        # data_dim = data_in.shape[-1]\n",
    "        \n",
    "        \n",
    "tfnetwork = TransformerClassifier(100, dmodel = 36, nhead = 6, dhid = 100, nlayers = 3, data_dim = 5, nclasses = 3)\n",
    "\n",
    "tfnetwork(tf_test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "a431a705-bac6-4aef-82b7-584d9564aea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 100, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "TransformerClassifier                         [5, 3]                    12,808\n",
       "├─Linear: 1-1                                 [5, 100, 36]              72\n",
       "├─Linear: 1-2                                 [5, 100, 36]              180\n",
       "├─TransformerEncoder: 1-3                     [5, 100, 36]              --\n",
       "│    └─ModuleList: 2-1                        --                        --\n",
       "│    │    └─TransformerEncoderLayer: 3-1      [5, 100, 36]              12,808\n",
       "│    │    └─TransformerEncoderLayer: 3-2      [5, 100, 36]              12,808\n",
       "│    │    └─TransformerEncoderLayer: 3-3      [5, 100, 36]              12,808\n",
       "├─Linear: 1-4                                 [5, 3]                    111\n",
       "===============================================================================================\n",
       "Total params: 51,595\n",
       "Trainable params: 51,595\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.11\n",
       "===============================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 2.78\n",
       "Params size (MB): 0.09\n",
       "Estimated Total Size (MB): 2.89\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "print(tuple(tf_test.shape))\n",
    "summary(tfnetwork, input_size = (5, 100, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "089a8d61-6722-4d33-b226-8f0c7c4af463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1145, 0.7564, 0.1291],\n",
      "        [0.1723, 0.6622, 0.1655],\n",
      "        [0.1269, 0.6999, 0.1732],\n",
      "        [0.1711, 0.6840, 0.1449],\n",
      "        [0.2155, 0.6460, 0.1385]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64)\n",
      "tensor([False, False,  True,  True,  True])\n",
      "tensor(3)\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(dataloader))\n",
    "\n",
    "tfnetwork = TransformerClassifier(100, dmodel = 36, nhead = 6, dhid = 100, nlayers = 3, data_dim = 5, nclasses = 3)\n",
    "\n",
    "train_out = tfnetwork(train_features)\n",
    "\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(tfnetwork.parameters(), lr = 0.001)\n",
    "\n",
    "print(train_out)\n",
    "print(train_labels)\n",
    "pred = torch.argmax(train_out, dim = 1)\n",
    "actual = torch.argmax(train_labels, dim = 1)\n",
    "\n",
    "print(pred == actual)\n",
    "print(torch.sum(pred == actual))\n",
    "tfnetwork.train()\n",
    "loss = loss_fn(train_out, train_labels)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "# tf_train\n",
    "# tf_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "2b32d268-f2d2-4a29-bae5-14c9f8a07232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerClassifier(\n",
       "  (positional_layer): Linear(in_features=1, out_features=36, bias=True)\n",
       "  (embed_layer): Linear(in_features=4, out_features=36, bias=True)\n",
       "  (encoderlayer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=36, out_features=100, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=100, out_features=36, bias=True)\n",
       "    (norm1): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=36, out_features=100, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=100, out_features=36, bias=True)\n",
       "        (norm1): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=36, out_features=100, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=100, out_features=36, bias=True)\n",
       "        (norm1): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=36, out_features=100, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=100, out_features=36, bias=True)\n",
       "        (norm1): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (class_encoder): Linear(in_features=36, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfnetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "96f781e1-74da-46fc-a9bc-717cd19f2ffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 36])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfnetwork_out = tfnetwork(tf_test)\n",
    "torch.max(tfnetwork_out,dim = 1)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "785e5e8d-7579-4edf-8597-ada7443140e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-inf, -inf, -inf, -inf],\n",
       "        [0., -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf],\n",
       "        [0., 0., 0., -inf]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(torch.ones(4, 4) * float('-inf'), diagonal=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "afcaad30-981b-4a75-a214-de491bf2757c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac10e44-7ccd-4f4e-ab48-255648980338",
   "metadata": {},
   "source": [
    "## Old S2 pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8801cb4f-7504-4665-bd51-91a431e0d23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class s2Dataset(Dataset):\n",
    "#     \"\"\"Sentinel 2 dataset\"\"\"\n",
    "    \n",
    "#     def __init__(self, proj_path, class_colname):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             proj_path (string): path to manclassify project\n",
    "#         \"\"\"\n",
    "#         self.proj_path = proj_path\n",
    "#         proj_normpath = os.path.normpath(proj_path)\n",
    "#         proj_dirname = proj_normpath.split(os.sep)[-1]\n",
    "#         self.proj_name = re.sub(\"_classification$\",\"\",proj_dirname)\n",
    "#         self.class_path = os.path.join(proj_path, self.proj_name + \"_classification\")\n",
    "#         self.ts_path = os.path.join(proj_path, self.proj_name + \"_download_timeseries\")\n",
    "#         self.pt_classes = pd.read_csv(os.path.join(self.class_path,\"location_classification.csv\"))\n",
    "#         self.pt_classes = classes[['loc_id', class_colname]].dropna()\n",
    "#         # self.pt_classes['loc_id'] = self.pt_classes['loc_id'] + 10.5 # for testing index only\n",
    "#         self.classes = pd.unique(self.pt_classes[class_colname])\n",
    "#         self.labels = self.pt_classes.assign(val = 1).pivot_table(columns = class_colname, index = 'loc_id', values = 'val', fill_value= 0)\n",
    "\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         loc_id = self.labels.index[idx]\n",
    "#         self.last_loc_id = loc_id\n",
    "        \n",
    "#         # select location id\n",
    "#         s2_ts_x = s2_ts[['B8','B4','B3','B2','day']]\n",
    "#         x = torch.tensor(s2_ts_x.to_numpy())\n",
    "        \n",
    "#         # get one-hot encoding for the point as tensor\n",
    "#         y = torch.tensor(self.labels.iloc[idx].to_numpy())\n",
    "        \n",
    "#         return x, y\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return self.pt_classes.shape[0]\n",
    "\n",
    "\n",
    "# proj_path = \"/Users/gopal/Google Drive/_Research/Research projects/ML/manclassify/app_data/Thailand\"\n",
    "# # date_rangeX = pd.to_datetime(['2019-06-01','2020-05-31'])\n",
    "# s2_dataset = s2Dataset(proj_path = proj_path, class_colname = 'Subclass2019')\n",
    "# x = s2_dataset.__getitem__(10)\n",
    "# sys.getsizeof(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlnightly",
   "language": "python",
   "name": "dlnightly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
