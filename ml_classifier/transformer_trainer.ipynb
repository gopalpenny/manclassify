{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7217f6c2-4c4c-4e5c-9bb6-8687ed8f5cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import re\n",
    "import sys\n",
    "from datetime import timedelta\n",
    "# from torch.nn.functional import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "1f95fcad-56ab-43b6-a50d-317152f7e186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loc_id</th>\n",
       "      <th>Subclass2019</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Plantation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Crop(Single)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Crop(Single)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Crop(Single)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Plantation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>496</td>\n",
       "      <td>Crop(Single)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>497</td>\n",
       "      <td>Crop(Single)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>498</td>\n",
       "      <td>Plantation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>499</td>\n",
       "      <td>Plantation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>500</td>\n",
       "      <td>Crop(Double)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>501 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     loc_id  Subclass2019\n",
       "0         0    Plantation\n",
       "1         1  Crop(Single)\n",
       "2         2  Crop(Single)\n",
       "3         3  Crop(Single)\n",
       "4         4    Plantation\n",
       "..      ...           ...\n",
       "496     496  Crop(Single)\n",
       "497     497  Crop(Single)\n",
       "498     498    Plantation\n",
       "499     499    Plantation\n",
       "500     500  Crop(Double)\n",
       "\n",
       "[501 rows x 2 columns]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "proj_paths = [\"/Users/gopal/Google Drive/_Research/Research projects/ML/manclassify/app_data/Thailand\",\n",
    "              \"/Users/gopalpenny/Library/CloudStorage/GoogleDrive-gopalpenny@gmail.com/My Drive/_Research/Research projects/ML/manclassify/app_data/Thailand\"]\n",
    "\n",
    "proj_path = [path for path in proj_paths if os.path.exists(path)][0]\n",
    "\n",
    "class_path = os.path.join(proj_path,\"Thailand_classification\")\n",
    "ts_path = os.path.join(proj_path,\"Thailand_download_timeseries\")\n",
    "# pd.read_csv(\"\n",
    "os.listdir(class_path)\n",
    "\n",
    "loc_id = 0\n",
    "\n",
    "s2_csv_name = f\"pt_ts_loc{loc_id}_s2.csv\"\n",
    "s2_csv_name\n",
    "\n",
    "class_colname = 'Subclass2019'\n",
    "\n",
    "proj_normpath = os.path.normpath(proj_path)\n",
    "proj_dirname = proj_normpath.split(os.sep)[-1]\n",
    "proj_name = re.sub(\"_classification$\",\"\",proj_dirname)\n",
    "class_path = os.path.join(proj_path, proj_name + \"_classification\")\n",
    "ts_path = os.path.join(proj_path, proj_name + \"_download_timeseries\")\n",
    "pt_classes = pd.read_csv(os.path.join(class_path,\"location_classification.csv\"))\n",
    "pt_classes = pt_classes[['loc_id', class_colname]].dropna()\n",
    "\n",
    "pt_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadc6355-df65-4834-b219-1d3d6ead823f",
   "metadata": {},
   "source": [
    "## Generate the torch tensor dataset\n",
    "\n",
    "### Define function to read timeseries\n",
    "\n",
    "* Read timeseries\n",
    "* Filter timeseries to date range (+/- 60 days)\n",
    "* Remove observations with clouds\n",
    "* Take the mean value for each day (occurs when multiple overpasses happen on the same day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b32be994-4f4d-4666-9e96-9424cc71f3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep dataset\n",
    "date_range = pd.to_datetime(['2019-06-01','2020-05-31'])\n",
    "\n",
    "def prep_s2_loc(loc_id, date_range, proj_path):\n",
    "    ts_path = os.path.join(proj_path,\"Thailand_download_timeseries\")\n",
    "    s2_csv_name = f\"pt_ts_loc{loc_id}_s2.csv\"\n",
    "    s2_csv_path = os.path.join(ts_path, s2_csv_name)\n",
    "    s2_ts = pd.read_csv(s2_csv_path)\n",
    "\n",
    "    # extract dates from image ids\n",
    "    s2_ts['datestr'] = [re.sub(\"(^[0-9]+)[a-zA-Z].*\",\"\\\\1\",x) for x in s2_ts.image_id]\n",
    "    s2_ts['date'] = pd.to_datetime(s2_ts.datestr, format = \"%Y%m%d\")\n",
    "\n",
    "    # subset to cloud-free days AND within date_range\n",
    "    s2_ts = s2_ts[(s2_ts.date >= date_range[0] - timedelta(days = 60)) & \n",
    "                  (s2_ts.date <= date_range[1] + timedelta(days = 60)) & \n",
    "                  (s2_ts.cloudmask == 0)]\n",
    "\n",
    "    # calculate day from startday\n",
    "    date_diff = (s2_ts.date - date_range[0])\n",
    "    s2_ts['day'] = [x.days for x in date_diff]\n",
    "    s2_ts['loc_id'] = loc_id\n",
    "\n",
    "    # select only predictor and position columns, return tensor\n",
    "    s2_ts_x = s2_ts[['loc_id','day','B8','B4','B3','B2']]\n",
    "    return s2_ts_x\n",
    "\n",
    "# s2_ts_loc125 = prep_s2_loc(125, date_range, proj_path)\n",
    "# s2_ts_loc125.groupby(['loc_id','day'],as_index = False).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6336b48f-e6ff-468b-95cf-a1396e0e1768",
   "metadata": {},
   "source": [
    "### Get the torch tensor dataset (prep and save OR read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "38ac523f-49d7-4427-8d02-85e905ab73f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from ipywidgets import IntProgress\n",
    "# from IPython.display import display\n",
    "\n",
    "if os.path.exists(os.path.join(proj_path, 's2_ts_prepped.pt')):\n",
    "    loc_ts_tor = torch.load(os.path.join(proj_path, 's2_ts_prepped.pt'))\n",
    "    \n",
    "else:\n",
    "    # f = IntProgress(min=0, max=pt_classes.shape[0]) # instantiate the bar\n",
    "    display(f) # display the bar\n",
    "    \n",
    "    s2_ts_list = []\n",
    "    loc_id_list = []\n",
    "    for i in np.arange(pt_classes.shape[0]):\n",
    "        # loc_id = 499\n",
    "        # print(loc_id)\n",
    "        loc_id = pt_classes.loc_id.iloc[i]\n",
    "        # loc_id_list.append(loc_id)\n",
    "        s2_ts_loc = prep_s2_loc(loc_id, date_range, proj_path)\n",
    "        s2_ts_loc = s2_ts_loc.groupby(['loc_id','day'],as_index = False).mean()\n",
    "        s2_ts_tor = torch.tensor(s2_ts_loc.to_numpy())\n",
    "        s2_ts_list.append(s2_ts_tor)\n",
    "        # f.value += 1\n",
    "        \n",
    "    loc_ts_tor = torch.cat(s2_ts_list)\n",
    "\n",
    "    torch.save(loc_ts_tor, os.path.join(proj_path, 's2_ts_prepped.pt'))\n",
    "\n",
    "sys.getsizeof(loc_ts_tor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87657506-61f2-446c-ab16-6d6a4b200b16",
   "metadata": {},
   "source": [
    "### Prep the dataset tensors\n",
    "\n",
    "* Subset to training classes (crops & plantations)\n",
    "* Check max number of rows\n",
    "* Normalize & center\n",
    "* Split loc_id into training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7b06a55-3c55-42e7-91fa-6ca3ea412559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All classes\n",
      "              loc_id\n",
      "Subclass2019        \n",
      "Crop(Double)      68\n",
      "Crop(Single)     278\n",
      "Forest             3\n",
      "Golf               1\n",
      "Mixed             20\n",
      "Plantation       109\n",
      "Unsure            17\n",
      "Urban              1\n",
      "Water              4\n",
      "\n",
      "Training dataset (pt_classes_ag)\n",
      "      loc_id  Subclass2019\n",
      "0         0    Plantation\n",
      "1         1  Crop(Single)\n",
      "2         2  Crop(Single)\n",
      "3         3  Crop(Single)\n",
      "4         4    Plantation\n",
      "..      ...           ...\n",
      "496     496  Crop(Single)\n",
      "497     497  Crop(Single)\n",
      "498     498    Plantation\n",
      "499     499    Plantation\n",
      "500     500  Crop(Double)\n",
      "\n",
      "[455 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print('All classes')\n",
    "print(pt_classes.groupby('Subclass2019').count())\n",
    "\n",
    "train_classes = ['Crop(Double)','Crop(Single)','Plantation']\n",
    "pt_classes_ag = pt_classes[pt_classes['Subclass2019'].isin(train_classes)]\n",
    "print('\\nTraining dataset (pt_classes_ag)\\n',pt_classes_ag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "d2a727f0-a8c6-415c-b482-e19f70c5f571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of observations for any loc_id\n",
      "     loc_id  num_obs\n",
      "481     481       94\n"
     ]
    }
   ],
   "source": [
    "loc_ts_tor = loc_ts_tor[(loc_ts_tor[:,1] >= -30) & (loc_ts_tor[:,1] <= 395)]\n",
    "\n",
    "row_means= loc_ts_tor.mean(dim = 1)#.shape #.unsqueeze(0).repeat(5,1)\n",
    "loc_ts_tor = loc_ts_tor[~torch.isnan(row_means)]\n",
    "col_means= loc_ts_tor.mean(dim = 0)#.shape #.unsqueeze(0).repeat(5,1)\n",
    "col_std= loc_ts_tor.std(dim = 0)#.shape #.unsqueeze(0).repeat(5,1)\n",
    "col_means[[0,1]] = 0\n",
    "col_std[[0,1]] = 1\n",
    "\n",
    "loc_ts_tor_std = col_std.unsqueeze(0).repeat(loc_ts_tor.shape[0],1)\n",
    "loc_ts_tor_mean = col_means.unsqueeze(0).repeat(loc_ts_tor.shape[0],1)\n",
    "\n",
    "loc_ts_norm = (loc_ts_tor - loc_ts_tor_mean) / loc_ts_tor_std\n",
    "\n",
    "# get max of number of observations per location\n",
    "# idx = np.arange(loc_ts_norm.shape[0])\n",
    "loc_id = np.unique(loc_ts_norm[:,0])\n",
    "num_obs = pd.DataFrame({'loc_id' : np.unique(loc_ts_norm[:,0]).astype('int')})\n",
    "num_obs['num_obs'] = [loc_ts_norm[loc_ts_norm[:,0]==i,:].shape[0] for i in num_obs['loc_id']]\n",
    "print(\"Max number of observations for any loc_id\")\n",
    "print(num_obs.iloc[[num_obs['num_obs'].idxmax()]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "bd6b44a2-91e1-4bfd-8b55-cdba3b3f1f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (loc_train summary)\n",
      "               loc_id    n  weight\n",
      "Subclass2019                     \n",
      "Crop(Double)      54   54      54\n",
      "Crop(Single)     222  222     222\n",
      "Plantation        87   87      87\n",
      "\n",
      "Testing (loc_test summary)\n",
      "               loc_id\n",
      "Subclass2019        \n",
      "Crop(Double)      14\n",
      "Crop(Single)      56\n",
      "Plantation        22\n"
     ]
    }
   ],
   "source": [
    "loc_train = pt_classes_ag.groupby('Subclass2019', group_keys = False).apply(lambda x: x.sample(frac = 0.8))\n",
    "loc_train['n'] = loc_train.groupby('Subclass2019')['loc_id'].transform(len)\n",
    "loc_train['weight'] = loc_train.shape[0] / loc_train['n'] \n",
    "\n",
    "loc_test = pt_classes_ag[~pt_classes_ag['loc_id'].isin(loc_train.loc_id)]\n",
    "print('Training (loc_train summary)\\n', loc_train.groupby('Subclass2019').count())\n",
    "print('\\nTesting (loc_test summary)\\n', loc_test.groupby('Subclass2019').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "0bdda10b-384d-45fa-96d5-e83ec73a93f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loc_id</th>\n",
       "      <th>Subclass2019</th>\n",
       "      <th>n</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>395</td>\n",
       "      <td>Crop(Double)</td>\n",
       "      <td>54</td>\n",
       "      <td>6.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>260</td>\n",
       "      <td>Crop(Double)</td>\n",
       "      <td>54</td>\n",
       "      <td>6.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>65</td>\n",
       "      <td>Crop(Double)</td>\n",
       "      <td>54</td>\n",
       "      <td>6.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>378</td>\n",
       "      <td>Crop(Double)</td>\n",
       "      <td>54</td>\n",
       "      <td>6.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>172</td>\n",
       "      <td>Crop(Double)</td>\n",
       "      <td>54</td>\n",
       "      <td>6.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>118</td>\n",
       "      <td>Plantation</td>\n",
       "      <td>87</td>\n",
       "      <td>4.172414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>298</td>\n",
       "      <td>Plantation</td>\n",
       "      <td>87</td>\n",
       "      <td>4.172414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>293</td>\n",
       "      <td>Plantation</td>\n",
       "      <td>87</td>\n",
       "      <td>4.172414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>303</td>\n",
       "      <td>Plantation</td>\n",
       "      <td>87</td>\n",
       "      <td>4.172414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>482</td>\n",
       "      <td>Plantation</td>\n",
       "      <td>87</td>\n",
       "      <td>4.172414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>363 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     loc_id  Subclass2019   n    weight\n",
       "395     395  Crop(Double)  54  6.722222\n",
       "260     260  Crop(Double)  54  6.722222\n",
       "65       65  Crop(Double)  54  6.722222\n",
       "378     378  Crop(Double)  54  6.722222\n",
       "172     172  Crop(Double)  54  6.722222\n",
       "..      ...           ...  ..       ...\n",
       "118     118    Plantation  87  4.172414\n",
       "298     298    Plantation  87  4.172414\n",
       "293     293    Plantation  87  4.172414\n",
       "303     303    Plantation  87  4.172414\n",
       "482     482    Plantation  87  4.172414\n",
       "\n",
       "[363 rows x 4 columns]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c10ca5-4adb-4726-bc9a-792401378301",
   "metadata": {},
   "source": [
    "## Prepare the S2 dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "848fb5a1-70fc-4de4-8d58-ebaea6adaa25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class s2Dataset(Dataset):\n",
    "    \"\"\"Sentinel 2 dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, x, y, max_obs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_train (tensor): contains loc_id and predictors as columns, s2 observations as rows\n",
    "            y_train (tensor): contains loc_id as rows, weights and class as 1-hot columns\n",
    "        \"\"\"\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.max_obs = max_obs\n",
    "        # self.proj_path = proj_path\n",
    "        # proj_normpath = os.path.normpath(proj_path)\n",
    "        # proj_dirname = proj_normpath.split(os.sep)[-1]\n",
    "        # self.proj_name = re.sub(\"_classification$\",\"\",proj_dirname)\n",
    "        # self.class_path = os.path.join(proj_path, self.proj_name + \"_classification\")\n",
    "        # self.ts_path = os.path.join(proj_path, self.proj_name + \"_download_timeseries\")\n",
    "        # self.pt_classes = pd.read_csv(os.path.join(self.class_path,\"location_classification.csv\"))\n",
    "        # self.pt_classes = classes[['loc_id', class_colname]].dropna()\n",
    "        # self.classes = pd.unique(self.pt_classes[class_colname])\n",
    "        # self.labels = self.pt_classes.assign(val = 1).pivot_table(columns = class_colname, index = 'loc_id', values = 'val', fill_value= 0)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # get loc_id\n",
    "        loc_id = self.y_train[idx,0]\n",
    "        self.last_loc_id = loc_id\n",
    "        \n",
    "        # select location id\n",
    "        x_loc = self.x_train[self.x_train[:,0]==loc_id]\n",
    "        x_prep = x_loc[:,1:] # remove loc_id column\n",
    "        \n",
    "        # pad zeros to max_obs\n",
    "        n_pad = self.max_obs - x_prep.shape[0]\n",
    "        \n",
    "        x = torch.cat((x_prep, torch.zeros(n_pad, x_prep.shape[1])), dim = 0)\n",
    "        \n",
    "        x = x.float()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # get one-hot encoding for the point as tensor\n",
    "        y = torch.tensor(y_train[idx,1:])\n",
    "        \n",
    "        return x, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "064c8db3-3b4b-49fa-a8c6-a4862ceff953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loc_id</th>\n",
       "      <th>Subclass2019</th>\n",
       "      <th>n</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>395</td>\n",
       "      <td>Crop(Double)</td>\n",
       "      <td>54</td>\n",
       "      <td>6.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>260</td>\n",
       "      <td>Crop(Double)</td>\n",
       "      <td>54</td>\n",
       "      <td>6.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>65</td>\n",
       "      <td>Crop(Double)</td>\n",
       "      <td>54</td>\n",
       "      <td>6.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>378</td>\n",
       "      <td>Crop(Double)</td>\n",
       "      <td>54</td>\n",
       "      <td>6.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>172</td>\n",
       "      <td>Crop(Double)</td>\n",
       "      <td>54</td>\n",
       "      <td>6.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>118</td>\n",
       "      <td>Plantation</td>\n",
       "      <td>87</td>\n",
       "      <td>4.172414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>298</td>\n",
       "      <td>Plantation</td>\n",
       "      <td>87</td>\n",
       "      <td>4.172414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>293</td>\n",
       "      <td>Plantation</td>\n",
       "      <td>87</td>\n",
       "      <td>4.172414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>303</td>\n",
       "      <td>Plantation</td>\n",
       "      <td>87</td>\n",
       "      <td>4.172414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>482</td>\n",
       "      <td>Plantation</td>\n",
       "      <td>87</td>\n",
       "      <td>4.172414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>363 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     loc_id  Subclass2019   n    weight\n",
       "395     395  Crop(Double)  54  6.722222\n",
       "260     260  Crop(Double)  54  6.722222\n",
       "65       65  Crop(Double)  54  6.722222\n",
       "378     378  Crop(Double)  54  6.722222\n",
       "172     172  Crop(Double)  54  6.722222\n",
       "..      ...           ...  ..       ...\n",
       "118     118    Plantation  87  4.172414\n",
       "298     298    Plantation  87  4.172414\n",
       "293     293    Plantation  87  4.172414\n",
       "303     303    Plantation  87  4.172414\n",
       "482     482    Plantation  87  4.172414\n",
       "\n",
       "[363 rows x 4 columns]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.random.manual_seed(100)\n",
    "loc_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90680b25-01d0-4c4e-972d-034ea76b5b03",
   "metadata": {
    "tags": []
   },
   "source": [
    "### get training data\n",
    "\n",
    "* `y_train` directly from `loc_train` & pivot\n",
    "* `x_train` from `loc_ts_norm`, subset to `y_train[:,0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "58824da7-827c-430e-9954-0b03a8a80277",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train:\n",
      " [[  0   0   0   1]\n",
      " [  1   0   1   0]\n",
      " [  2   0   1   0]\n",
      " ...\n",
      " [496   0   1   0]\n",
      " [497   0   1   0]\n",
      " [499   0   0   1]]\n",
      "     loc_id  Subclass2019\n",
      "4         4    Plantation\n",
      "5         5  Crop(Double)\n",
      "7         7    Plantation\n",
      "11       11  Crop(Single)\n",
      "24       24  Crop(Single)\n",
      "..      ...           ...\n",
      "471     471  Crop(Double)\n",
      "474     474  Crop(Single)\n",
      "492     492    Plantation\n",
      "498     498    Plantation\n",
      "500     500  Crop(Double)\n",
      "\n",
      "[92 rows x 2 columns]\n",
      "y_test:\n",
      " [[ 4  0  0  1]\n",
      " [ 5  1  0  0]\n",
      " [ 7  0  0  1]\n",
      " [11  0  1  0]\n",
      " [24  0  1  0]\n",
      " [29  0  1  0]\n",
      " [30  0  1  0]\n",
      " [39  0  0  1]\n",
      " [41  1  0  0]\n",
      " [48  1  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# get y_train values from loc_train\n",
    "y_train_df = (loc_train.assign(val = 1) \\\n",
    "  .pivot_table(columns = class_colname, index = ['loc_id'], values = 'val', fill_value= 0) \\\n",
    "  .reset_index(['loc_id']))\n",
    "y_train = y_train_df.to_numpy()\n",
    "print('y_train:\\n',y_train)\n",
    "\n",
    "# get x_train values from loc_ts_norm (based on loc_id)\n",
    "x_train = loc_ts_norm[torch.isin(loc_ts_norm[:,0],torch.tensor(y_train[:,0]).to(torch.float64)),:]\n",
    "\n",
    "print(loc_test)\n",
    "# get y_test values from loc_test\n",
    "y_test_df = (loc_test.assign(val = 1) \\\n",
    "  .pivot_table(columns = class_colname, index = ['loc_id'], values = 'val', fill_value= 0) \\\n",
    "  .reset_index(['loc_id']))\n",
    "y_test = y_test_df.to_numpy()\n",
    "print('y_test:\\n',y_test[0:10,])\n",
    "\n",
    "# get x_train values from loc_ts_norm (based on loc_id)\n",
    "x_test = loc_ts_norm[torch.isin(loc_ts_norm[:,0],torch.tensor(y_test[:,0]).to(torch.float64)),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c631a3d7-b607-474a-a6ef-6a8751ef1aad",
   "metadata": {},
   "source": [
    "### build pytorch dataset: `s2_dateset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "43ba874b-c3df-4eed-9511-8103df128f75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x example, shape: torch.Size([100, 5]) \n",
      "(idx=2) columns: day, B8, B4, B3, B2\n",
      " tensor([[-1.2000e+01,  1.2615e+00, -2.6081e-01,  3.0071e-01,  5.2696e-02],\n",
      "        [-7.0000e+00, -3.9504e-01,  3.6840e-01,  6.7323e-01,  7.4909e-01],\n",
      "        [ 3.0000e+00,  4.5408e-02,  1.8113e+00,  2.3523e+00,  2.2565e+00],\n",
      "        [ 1.0800e+02, -1.0164e+00, -8.9497e-01, -7.8398e-01, -5.6436e-01],\n",
      "        [ 1.1800e+02, -1.7902e-01, -1.1190e+00, -9.3737e-01, -9.5810e-01],\n",
      "        [ 1.2300e+02, -4.3852e-01, -1.1486e+00, -9.7298e-01, -8.8758e-01],\n",
      "        [ 1.2800e+02,  1.0011e-01, -1.1602e+00, -8.7985e-01, -1.0991e+00],\n",
      "        [ 1.3300e+02,  4.2694e-01, -9.9709e-01, -5.8403e-01, -3.2929e-01],\n",
      "        [ 1.4300e+02,  4.5780e-01, -1.3430e+00, -1.3044e+00, -1.2813e+00],\n",
      "        [ 1.6300e+02,  7.8603e-01, -6.9072e-01, -9.6463e-02, -6.3488e-01],\n",
      "        [ 1.6800e+02,  5.8545e-01,  8.6584e-01,  1.0786e+00,  7.2852e-01],\n",
      "        [ 1.7300e+02, -5.0893e-03,  8.2136e-01,  5.0888e-01,  3.4947e-01],\n",
      "        [ 1.7800e+02, -2.8703e-01,  5.8911e-01,  8.9797e-02, -1.4417e-01],\n",
      "        [ 1.8300e+02, -2.7862e-01,  6.2041e-01,  1.8567e-01,  9.3833e-02],\n",
      "        [ 1.8800e+02, -1.6219e-01,  5.6111e-01,  1.4184e-01, -1.9465e-04],\n",
      "        [ 1.9300e+02, -1.8604e-01,  5.6605e-01,  2.7058e-01,  3.2302e-01],\n",
      "        [ 1.9800e+02, -3.2350e-01,  5.1170e-01, -8.8114e-03, -1.5593e-01],\n",
      "        [ 2.0300e+02, -5.6897e-01,  3.7499e-01, -1.4290e-02,  1.3203e-01],\n",
      "        [ 2.0800e+02, -1.5635e+00, -2.7070e-01, -8.3876e-01, -7.7004e-01],\n",
      "        [ 2.1300e+02, -9.0654e-02,  7.8018e-01,  5.7462e-01,  5.4046e-01],\n",
      "        [ 2.1800e+02, -6.5875e-01,  3.4534e-01, -5.5376e-02, -4.4270e-02],\n",
      "        [ 2.2300e+02, -8.8037e-01,  1.9215e-01, -2.7724e-01, -1.0010e-01],\n",
      "        [ 2.2800e+02, -7.7657e-01,  4.7381e-01,  5.2258e-01,  9.0482e-01],\n",
      "        [ 2.4300e+02, -2.9264e-01,  6.0229e-01,  1.7471e-01,  1.8492e-01],\n",
      "        [ 2.4800e+02, -4.7079e-01,  7.8183e-01,  9.0605e-01,  1.1898e+00],\n",
      "        [ 2.5800e+02, -3.3613e-01,  6.7312e-01,  1.9388e-01,  1.5260e-01],\n",
      "        [ 2.6300e+02, -3.1789e-01,  7.3736e-01,  4.6232e-01,  7.2558e-01],\n",
      "        [ 2.6800e+02, -1.9726e-01,  9.1360e-01,  7.8553e-01,  1.0106e+00],\n",
      "        [ 2.7300e+02, -3.2210e-01,  9.2349e-01,  1.1197e+00,  1.6394e+00],\n",
      "        [ 2.8300e+02, -1.5798e-01,  8.5925e-01,  5.4449e-01,  6.1392e-01],\n",
      "        [ 2.9800e+02, -1.0700e-02,  9.9925e-01,  7.5266e-01,  8.0785e-01],\n",
      "        [ 3.0300e+02,  3.1381e-02,  1.0833e+00,  9.4440e-01,  1.1663e+00],\n",
      "        [ 3.2300e+02, -3.9644e-01,  7.7030e-01,  3.4727e-01,  3.3184e-01],\n",
      "        [ 3.3300e+02,  2.4319e-01,  1.0042e+00,  1.3854e+00,  1.7893e+00],\n",
      "        [ 3.4800e+02,  1.2396e-01,  1.7554e-02,  3.2810e-01,  4.1118e-01],\n",
      "        [ 3.6800e+02,  3.5821e-01, -5.1283e-01, -3.4298e-01, -5.4967e-01],\n",
      "        [ 3.9300e+02, -5.1708e-01, -4.9636e-01, -3.8133e-01, -1.9706e-01],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
      "\n",
      "\n",
      "y example (idx=2): crops(double) crops(single) plantation\n",
      " tensor([0, 1, 0])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "s2_train = s2Dataset(x = x_train, y = y_train, max_obs = 100)\n",
    "s2_test = s2Dataset(x = x_test, y = y_test, max_obs = 100)\n",
    "\n",
    "# example item in dataset\n",
    "idx_test = 2\n",
    "x, y = s2_train.__getitem__(idx_test)\n",
    "\n",
    "print(f'x example, shape: {x.shape} \\n(idx={idx_test}) columns: day, B8, B4, B3, B2\\n',x)\n",
    "# print()\n",
    "print(f'\\n\\ny example (idx={idx_test}): crops(double) crops(single) plantation\\n',y)\n",
    "print(y.shape)\n",
    "# sys.getsizeof(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298322ee-b1f3-42a9-a7c9-de79fb131fdc",
   "metadata": {},
   "source": [
    "### generate sampling weights for data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c911f46-100f-424d-9c1f-c9982d0cd300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loc_id</th>\n",
       "      <th>Subclass2019</th>\n",
       "      <th>n</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>106</td>\n",
       "      <td>Crop(Double)</td>\n",
       "      <td>54</td>\n",
       "      <td>6.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>500</td>\n",
       "      <td>Crop(Double)</td>\n",
       "      <td>54</td>\n",
       "      <td>6.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>445</td>\n",
       "      <td>Crop(Double)</td>\n",
       "      <td>54</td>\n",
       "      <td>6.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>Crop(Double)</td>\n",
       "      <td>54</td>\n",
       "      <td>6.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>241</td>\n",
       "      <td>Crop(Double)</td>\n",
       "      <td>54</td>\n",
       "      <td>6.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>56</td>\n",
       "      <td>Plantation</td>\n",
       "      <td>87</td>\n",
       "      <td>4.172414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>274</td>\n",
       "      <td>Plantation</td>\n",
       "      <td>87</td>\n",
       "      <td>4.172414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>Plantation</td>\n",
       "      <td>87</td>\n",
       "      <td>4.172414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>380</td>\n",
       "      <td>Plantation</td>\n",
       "      <td>87</td>\n",
       "      <td>4.172414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>439</td>\n",
       "      <td>Plantation</td>\n",
       "      <td>87</td>\n",
       "      <td>4.172414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>363 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     loc_id  Subclass2019   n    weight\n",
       "106     106  Crop(Double)  54  6.722222\n",
       "500     500  Crop(Double)  54  6.722222\n",
       "445     445  Crop(Double)  54  6.722222\n",
       "41       41  Crop(Double)  54  6.722222\n",
       "241     241  Crop(Double)  54  6.722222\n",
       "..      ...           ...  ..       ...\n",
       "56       56    Plantation  87  4.172414\n",
       "274     274    Plantation  87  4.172414\n",
       "39       39    Plantation  87  4.172414\n",
       "380     380    Plantation  87  4.172414\n",
       "439     439    Plantation  87  4.172414\n",
       "\n",
       "[363 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc_train_n = loc_train\n",
    "loc_train_n['n'] = loc_train_n.groupby('Subclass2019')['loc_id'].transform(len)\n",
    "loc_train_n['weight'] = loc_train_n.shape[0] / loc_train_n['n'] \n",
    "loc_train_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "96ad31fc-257b-4aaf-b015-d6718ff5a8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes = torch.stack([torch.argmax(s2_train.__getitem__(i)[1]) for i in range(s2_train.__len__())])\n",
    "class_sample_count = np.array([torch.sum(torch.stack(l) == i) for i in torch.unique(torch.stack(l))])\n",
    "weight = 1. / class_sample_count\n",
    "sample_weights = np.array([weight[i] for i in target_classes])\n",
    "sampler = WeightedRandomSampler(weights = sample_weights, num_samples = len(sample_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "e1ad4e3d-3d90-45b7-8fcf-180c28fdcc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s2_train\n",
    "\n",
    "train_dl = DataLoader(s2_train, batch_size = 20, drop_last = True, sampler = sampler)\n",
    "test_dl = DataLoader(s2_test, batch_size = 20, drop_last = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "28c6c16a-7a3b-4167-ae61-22fca489c18c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6a2bbd30-e297-47cf-a025-d8983de6c8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i == 1:\n",
      " tensor([-13.0000,  -0.5409,  -0.0236,  -0.1732,  -0.0266])\n",
      "i == 10:\n",
      " tensor([-13.0000,  -0.3263,   0.8411,   0.8759,   0.7785])\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "for train, labels in train_dl:\n",
    "    if i == 1:\n",
    "        print(\"i == 1:\\n\",train[1, 1, :])\n",
    "    if i == 10:\n",
    "        print(\"i == 10:\\n\",train[1, 1, :])\n",
    "    i += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bb5ebc9-1110-4a56-a1e4-6546698d6905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 100, 5])\n",
      "tensor([[-30.0000,  -0.2786,  -1.3512,  -1.4605,  -1.3607],\n",
      "        [-25.0000,   0.9950,  -0.8620,  -0.6580,  -0.7025],\n",
      "        [ -5.0000,   2.9855,  -1.4253,  -1.1099,  -1.3930]])\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_dl))\n",
    "tf_test = train_features[:,:,:]\n",
    "# tf_test\n",
    "# train_labels\n",
    "# tf_test\n",
    "tf_test = tf_test.float()\n",
    "print(tf_test.shape)\n",
    "\n",
    "print(tf_test[0, 0:3, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81301d5a-edc6-4b75-a700-6903c00c522f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3318b320-8a22-4274-8b1f-057fdea17c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PositionalEncoding(nn.Module):\n",
    "\n",
    "#     def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "#         super().__init__()\n",
    "#         self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "#         position = torch.arange(max_len).unsqueeze(1)\n",
    "#         div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "#         pe = torch.zeros(max_len, 1, d_model)\n",
    "#         pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "#         self.register_buffer('pe', pe)\n",
    "\n",
    "#     def forward(self, x: Tensor) -> Tensor:\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "#         \"\"\"\n",
    "#         x = x + self.pe[:x.size(0)]\n",
    "#         return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "620c3037-9bfe-4bca-aafa-a3f086326271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "nhead = 6 # number of attention heads\n",
    "head_dim = 8 # dimension of each word for each attention head\n",
    "dmodel = nhead * head_dim # embed_dim -- each word (row) is embedded to this dimension then split\n",
    "# across the nhead attention heads\n",
    "\n",
    "data_in = tf_test[:, :, 1:] # select only the data\n",
    "positions = tf_test[:,:,0:1] # split out positional data\n",
    "data_dim = data_in.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9864c574-a237-4233-85ed-f4abda964aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5097])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(torch.tensor([5.2333e-01]))/torch.sum(torch.exp(torch.tensor([-1.3249e-01, 5.2333e-01, -2.9124e-01])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2cc3947-ba93-4b8c-81a1-75732d2cef06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn, Tensor\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, ntoken: int, dmodel: int, nhead: int, dhid: int, \n",
    "                 nlayers: int, data_dim: int, nclasses: int):\n",
    "        \"\"\"\n",
    "        data_dim: dimension of data (i.e., num of columns) including position as first dimension\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.positional_layer = nn.Linear(1, dmodel)\n",
    "        self.embed_layer = nn.Linear(data_dim - 1, dmodel) # transform data to embed dimension (dmodel)\n",
    "        \n",
    "        # dim_feedforward: https://stackoverflow.com/questions/68087780/pytorch-transformer-argument-dim-feedforward\n",
    "        # shortly: dim_feedforward is a hidden layer between two forward layers at the end of the encoder layer, passed for each word one-by-one\n",
    "        self.encoderlayer = nn.TransformerEncoderLayer(d_model = dmodel, nhead = nhead, dim_feedforward = dhid)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoderlayer, nlayers)\n",
    "        \n",
    "        self.num_params = ntoken * dmodel\n",
    "        \n",
    "        self.class_encoder = nn.Linear(dmodel, nclasses)\n",
    "    \n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        \n",
    "        positions = src[:, :, 0:1]\n",
    "        data = src[:, :, 1:]\n",
    "        pe = self.positional_layer(positions)\n",
    "        data_embed = self.embed_layer(data)\n",
    "        data_and_pe = pe + data_embed\n",
    "        encoder_out = self.encoder(data_and_pe)\n",
    "        \n",
    "        maxpool = torch.max(encoder_out,dim = 1)[0]\n",
    "        \n",
    "        # softmax ensures output of model is probability of class membership -- which sum to 1\n",
    "        # BUT this is already done with CrossEntropyLoss so it's not necessary for this loss function\n",
    "        classes = self.class_encoder(maxpool) #, dim = 1\n",
    "        \n",
    "        # classes = nn.functional.softmax(classes, 1) # don't use softmax with cross entropy loss... or do?\n",
    "        # don't: https://stackoverflow.com/questions/55675345/should-i-use-softmax-as-output-when-using-cross-entropy-loss-in-pytorch\n",
    "        # do: Machine Learning with Pytorch and Scikitlearn (p 471: Loss functions for classifiers) -- BUT NOT WITH CROSS ENTROPY LOSS (p478\n",
    "        \n",
    "        return classes\n",
    "\n",
    "        # data_in = tf_test[:, :, 1:] # select only the data\n",
    "        # positions = tf_test[:,:,0:1] # split out positional data\n",
    "        # data_dim = data_in.shape[-1]\n",
    "        \n",
    "        \n",
    "tfnetwork = TransformerClassifier(100, dmodel = 36, nhead = 6, dhid = 100, nlayers = 3, data_dim = 5, nclasses = 3)\n",
    "\n",
    "tfnetwork(tf_test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a431a705-bac6-4aef-82b7-584d9564aea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 100, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "TransformerClassifier                         [5, 3]                    12,808\n",
       "├─Linear: 1-1                                 [5, 100, 36]              72\n",
       "├─Linear: 1-2                                 [5, 100, 36]              180\n",
       "├─TransformerEncoder: 1-3                     [5, 100, 36]              --\n",
       "│    └─ModuleList: 2-1                        --                        --\n",
       "│    │    └─TransformerEncoderLayer: 3-1      [5, 100, 36]              12,808\n",
       "│    │    └─TransformerEncoderLayer: 3-2      [5, 100, 36]              12,808\n",
       "│    │    └─TransformerEncoderLayer: 3-3      [5, 100, 36]              12,808\n",
       "├─Linear: 1-4                                 [5, 3]                    111\n",
       "===============================================================================================\n",
       "Total params: 51,595\n",
       "Trainable params: 51,595\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.11\n",
       "===============================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 2.78\n",
       "Params size (MB): 0.09\n",
       "Estimated Total Size (MB): 2.89\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "print(tuple(tf_test.shape))\n",
    "summary(tfnetwork, input_size = (5, 100, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "089a8d61-6722-4d33-b226-8f0c7c4af463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3])\n",
      "accuracy: tensor(0.3500)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'accuracy: 0.3499999940395355'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_dl))\n",
    "\n",
    "tfnetwork = TransformerClassifier(100, dmodel = 36, nhead = 6, dhid = 100, nlayers = 3, data_dim = 5, nclasses = 3)\n",
    "\n",
    "train_out = tfnetwork(train_features)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(tfnetwork.parameters(), lr = 0.001)\n",
    "\n",
    "print(train_out.shape)\n",
    "\n",
    "def get_accuracy(train_out, train_labels):\n",
    "    pred = torch.argmax(train_out, dim = 1)\n",
    "    actual = torch.argmax(train_labels, dim = 1)\n",
    "    accuracy = torch.sum(pred == actual) / len(pred)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "accuracy = get_accuracy(train_out, train_labels)\n",
    "print('accuracy:', accuracy)\n",
    "tfnetwork.train()\n",
    "loss = loss_fn(train_out, train_labels)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "# tf_train\n",
    "# tf_test.shape\n",
    "f\"accuracy: {accuracy.item()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "18b3e62f-48e8-4e4a-b966-8c072b547129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: 0.7068, Accuracy: 0.7000\n",
      "Epoch [2/1000], Loss: 0.5087, Accuracy: 0.7500\n",
      "Epoch [3/1000], Loss: 0.4487, Accuracy: 0.8500\n",
      "Epoch [4/1000], Loss: 0.5219, Accuracy: 0.8000\n",
      "Epoch [5/1000], Loss: 0.5847, Accuracy: 0.7500\n",
      "Epoch [6/1000], Loss: 0.6648, Accuracy: 0.6500\n",
      "Epoch [7/1000], Loss: 0.4599, Accuracy: 0.8000\n",
      "Epoch [8/1000], Loss: 0.7216, Accuracy: 0.6500\n",
      "Epoch [9/1000], Loss: 0.7424, Accuracy: 0.7000\n",
      "Epoch [10/1000], Loss: 0.5869, Accuracy: 0.7000\n",
      "Epoch [11/1000], Loss: 0.7499, Accuracy: 0.5500\n",
      "Epoch [12/1000], Loss: 0.5878, Accuracy: 0.7000\n",
      "Epoch [13/1000], Loss: 0.5037, Accuracy: 0.7000\n",
      "Epoch [14/1000], Loss: 0.8597, Accuracy: 0.5000\n",
      "Epoch [15/1000], Loss: 0.6998, Accuracy: 0.6500\n",
      "Epoch [16/1000], Loss: 0.6764, Accuracy: 0.6500\n",
      "Epoch [17/1000], Loss: 0.6137, Accuracy: 0.6500\n",
      "Epoch [18/1000], Loss: 0.6700, Accuracy: 0.6500\n",
      "Epoch [19/1000], Loss: 0.7748, Accuracy: 0.6500\n",
      "Epoch [20/1000], Loss: 0.6687, Accuracy: 0.8000\n",
      "Epoch [21/1000], Loss: 0.6058, Accuracy: 0.7000\n",
      "Epoch [22/1000], Loss: 0.4707, Accuracy: 0.8500\n",
      "Epoch [23/1000], Loss: 0.4525, Accuracy: 0.7500\n",
      "Epoch [24/1000], Loss: 1.0671, Accuracy: 0.5000\n",
      "Epoch [25/1000], Loss: 0.7579, Accuracy: 0.6500\n",
      "Epoch [26/1000], Loss: 0.6634, Accuracy: 0.6000\n",
      "Epoch [27/1000], Loss: 0.6084, Accuracy: 0.7000\n",
      "Epoch [28/1000], Loss: 0.4362, Accuracy: 0.9000\n",
      "Epoch [29/1000], Loss: 0.6817, Accuracy: 0.7000\n",
      "Epoch [30/1000], Loss: 0.6817, Accuracy: 0.6500\n",
      "Epoch [31/1000], Loss: 0.7301, Accuracy: 0.6500\n",
      "Epoch [32/1000], Loss: 0.7203, Accuracy: 0.5000\n",
      "Epoch [33/1000], Loss: 0.5604, Accuracy: 0.8000\n",
      "Epoch [34/1000], Loss: 0.7020, Accuracy: 0.6000\n",
      "Epoch [35/1000], Loss: 0.5424, Accuracy: 0.5500\n",
      "Epoch [36/1000], Loss: 0.7201, Accuracy: 0.7000\n",
      "Epoch [37/1000], Loss: 0.6051, Accuracy: 0.7000\n",
      "Epoch [38/1000], Loss: 0.7040, Accuracy: 0.6000\n",
      "Epoch [39/1000], Loss: 0.5356, Accuracy: 0.8500\n",
      "Epoch [40/1000], Loss: 0.6117, Accuracy: 0.7500\n",
      "Epoch [41/1000], Loss: 1.1393, Accuracy: 0.4000\n",
      "Epoch [42/1000], Loss: 1.0796, Accuracy: 0.4000\n",
      "Epoch [43/1000], Loss: 0.6181, Accuracy: 0.8000\n",
      "Epoch [44/1000], Loss: 0.6119, Accuracy: 0.7500\n",
      "Epoch [45/1000], Loss: 0.6017, Accuracy: 0.6500\n",
      "Epoch [46/1000], Loss: 0.6203, Accuracy: 0.7500\n",
      "Epoch [47/1000], Loss: 0.6122, Accuracy: 0.7000\n",
      "Epoch [48/1000], Loss: 0.8997, Accuracy: 0.6500\n",
      "Epoch [49/1000], Loss: 0.9098, Accuracy: 0.6500\n",
      "Epoch [50/1000], Loss: 0.6155, Accuracy: 0.7500\n",
      "Epoch [51/1000], Loss: 0.4959, Accuracy: 0.7500\n",
      "Epoch [52/1000], Loss: 0.4940, Accuracy: 0.8000\n",
      "Epoch [53/1000], Loss: 0.5965, Accuracy: 0.6000\n",
      "Epoch [54/1000], Loss: 0.4703, Accuracy: 0.8500\n",
      "Epoch [55/1000], Loss: 0.5269, Accuracy: 0.7500\n",
      "Epoch [56/1000], Loss: 0.6660, Accuracy: 0.7500\n",
      "Epoch [57/1000], Loss: 0.5052, Accuracy: 0.8000\n",
      "Epoch [58/1000], Loss: 0.5445, Accuracy: 0.7000\n",
      "Epoch [59/1000], Loss: 0.5707, Accuracy: 0.7500\n",
      "Epoch [60/1000], Loss: 0.9642, Accuracy: 0.5000\n",
      "Epoch [61/1000], Loss: 0.8804, Accuracy: 0.6000\n",
      "Epoch [62/1000], Loss: 0.8877, Accuracy: 0.5000\n",
      "Epoch [63/1000], Loss: 0.8936, Accuracy: 0.5500\n",
      "Epoch [64/1000], Loss: 0.6990, Accuracy: 0.8500\n",
      "Epoch [65/1000], Loss: 0.3258, Accuracy: 0.8000\n",
      "Epoch [66/1000], Loss: 0.4676, Accuracy: 0.8000\n",
      "Epoch [67/1000], Loss: 0.4418, Accuracy: 0.8000\n",
      "Epoch [68/1000], Loss: 0.6748, Accuracy: 0.7500\n",
      "Epoch [69/1000], Loss: 0.6169, Accuracy: 0.8000\n",
      "Epoch [70/1000], Loss: 0.4057, Accuracy: 0.7500\n",
      "Epoch [71/1000], Loss: 0.5077, Accuracy: 0.8000\n",
      "Epoch [72/1000], Loss: 0.7685, Accuracy: 0.7000\n",
      "Epoch [73/1000], Loss: 0.8141, Accuracy: 0.6000\n",
      "Epoch [74/1000], Loss: 0.5910, Accuracy: 0.8000\n",
      "Epoch [75/1000], Loss: 0.5152, Accuracy: 0.7500\n",
      "Epoch [76/1000], Loss: 0.5806, Accuracy: 0.7000\n",
      "Epoch [77/1000], Loss: 0.5961, Accuracy: 0.7000\n",
      "Epoch [78/1000], Loss: 0.3721, Accuracy: 0.8500\n",
      "Epoch [79/1000], Loss: 0.7258, Accuracy: 0.7500\n",
      "Epoch [80/1000], Loss: 0.4536, Accuracy: 0.8500\n",
      "Epoch [81/1000], Loss: 0.2241, Accuracy: 0.8500\n",
      "Epoch [82/1000], Loss: 0.7012, Accuracy: 0.6000\n",
      "Epoch [83/1000], Loss: 0.4094, Accuracy: 0.7000\n",
      "Epoch [84/1000], Loss: 0.6497, Accuracy: 0.6000\n",
      "Epoch [85/1000], Loss: 0.4716, Accuracy: 0.8000\n",
      "Epoch [86/1000], Loss: 0.5141, Accuracy: 0.8000\n",
      "Epoch [87/1000], Loss: 0.7990, Accuracy: 0.6500\n",
      "Epoch [88/1000], Loss: 0.4621, Accuracy: 0.8000\n",
      "Epoch [89/1000], Loss: 0.3849, Accuracy: 0.8000\n",
      "Epoch [90/1000], Loss: 0.6548, Accuracy: 0.7000\n",
      "Epoch [91/1000], Loss: 0.4594, Accuracy: 0.7500\n",
      "Epoch [92/1000], Loss: 0.4577, Accuracy: 0.8000\n",
      "Epoch [93/1000], Loss: 0.3853, Accuracy: 0.9000\n",
      "Epoch [94/1000], Loss: 0.3698, Accuracy: 0.8000\n",
      "Epoch [95/1000], Loss: 0.5417, Accuracy: 0.7000\n",
      "Epoch [96/1000], Loss: 0.7512, Accuracy: 0.7500\n",
      "Epoch [97/1000], Loss: 0.5955, Accuracy: 0.6500\n",
      "Epoch [98/1000], Loss: 0.6308, Accuracy: 0.7500\n",
      "Epoch [99/1000], Loss: 0.6682, Accuracy: 0.7500\n",
      "Epoch [100/1000], Loss: 0.6004, Accuracy: 0.8500\n",
      "Epoch [101/1000], Loss: 0.4836, Accuracy: 0.8000\n",
      "Epoch [102/1000], Loss: 0.3405, Accuracy: 0.9000\n",
      "Epoch [103/1000], Loss: 0.9123, Accuracy: 0.8000\n",
      "Epoch [104/1000], Loss: 0.4230, Accuracy: 0.9000\n",
      "Epoch [105/1000], Loss: 0.3877, Accuracy: 0.8000\n",
      "Epoch [106/1000], Loss: 0.4039, Accuracy: 0.8000\n",
      "Epoch [107/1000], Loss: 0.5759, Accuracy: 0.7500\n",
      "Epoch [108/1000], Loss: 0.3589, Accuracy: 0.8500\n",
      "Epoch [109/1000], Loss: 0.4388, Accuracy: 0.8500\n",
      "Epoch [110/1000], Loss: 0.4592, Accuracy: 0.8500\n",
      "Epoch [111/1000], Loss: 0.3252, Accuracy: 0.9000\n",
      "Epoch [112/1000], Loss: 0.2249, Accuracy: 0.9000\n",
      "Epoch [113/1000], Loss: 0.4507, Accuracy: 0.8000\n",
      "Epoch [114/1000], Loss: 0.4096, Accuracy: 0.9000\n",
      "Epoch [115/1000], Loss: 0.6138, Accuracy: 0.7000\n",
      "Epoch [116/1000], Loss: 0.5377, Accuracy: 0.8000\n",
      "Epoch [117/1000], Loss: 0.3740, Accuracy: 0.9000\n",
      "Epoch [118/1000], Loss: 0.5578, Accuracy: 0.7000\n",
      "Epoch [119/1000], Loss: 0.4380, Accuracy: 0.8000\n",
      "Epoch [120/1000], Loss: 0.3390, Accuracy: 0.8000\n",
      "Epoch [121/1000], Loss: 0.2801, Accuracy: 0.9000\n",
      "Epoch [122/1000], Loss: 0.3737, Accuracy: 0.8500\n",
      "Epoch [123/1000], Loss: 0.4175, Accuracy: 0.8000\n",
      "Epoch [124/1000], Loss: 0.5778, Accuracy: 0.7000\n",
      "Epoch [125/1000], Loss: 0.6019, Accuracy: 0.6500\n",
      "Epoch [126/1000], Loss: 0.6957, Accuracy: 0.5000\n",
      "Epoch [127/1000], Loss: 0.3445, Accuracy: 0.8500\n",
      "Epoch [128/1000], Loss: 0.3401, Accuracy: 0.8500\n",
      "Epoch [129/1000], Loss: 0.2248, Accuracy: 0.9500\n",
      "Epoch [130/1000], Loss: 0.5969, Accuracy: 0.6500\n",
      "Epoch [131/1000], Loss: 0.2870, Accuracy: 0.9000\n",
      "Epoch [132/1000], Loss: 0.5157, Accuracy: 0.7500\n",
      "Epoch [133/1000], Loss: 0.7588, Accuracy: 0.7000\n",
      "Epoch [134/1000], Loss: 0.2606, Accuracy: 0.9000\n",
      "Epoch [135/1000], Loss: 0.4026, Accuracy: 0.8500\n",
      "Epoch [136/1000], Loss: 0.3956, Accuracy: 0.8500\n",
      "Epoch [137/1000], Loss: 0.1395, Accuracy: 0.9500\n",
      "Epoch [138/1000], Loss: 0.7508, Accuracy: 0.6000\n",
      "Epoch [139/1000], Loss: 0.4928, Accuracy: 0.7000\n",
      "Epoch [140/1000], Loss: 0.7438, Accuracy: 0.8000\n",
      "Epoch [141/1000], Loss: 0.4381, Accuracy: 0.8500\n",
      "Epoch [142/1000], Loss: 0.4608, Accuracy: 0.8500\n",
      "Epoch [143/1000], Loss: 0.2775, Accuracy: 0.8500\n",
      "Epoch [144/1000], Loss: 0.3335, Accuracy: 0.8500\n",
      "Epoch [145/1000], Loss: 0.7023, Accuracy: 0.8000\n",
      "Epoch [146/1000], Loss: 0.5042, Accuracy: 0.7500\n",
      "Epoch [147/1000], Loss: 0.5238, Accuracy: 0.7500\n",
      "Epoch [148/1000], Loss: 0.5913, Accuracy: 0.7500\n",
      "Epoch [149/1000], Loss: 0.3763, Accuracy: 0.9000\n",
      "Epoch [150/1000], Loss: 0.2349, Accuracy: 0.9500\n",
      "Epoch [151/1000], Loss: 0.3038, Accuracy: 0.8500\n",
      "Epoch [152/1000], Loss: 0.7520, Accuracy: 0.7000\n",
      "Epoch [153/1000], Loss: 0.3128, Accuracy: 0.9000\n",
      "Epoch [154/1000], Loss: 0.2428, Accuracy: 0.9000\n",
      "Epoch [155/1000], Loss: 0.6997, Accuracy: 0.6500\n",
      "Epoch [156/1000], Loss: 0.3359, Accuracy: 0.8500\n",
      "Epoch [157/1000], Loss: 0.2766, Accuracy: 0.9500\n",
      "Epoch [158/1000], Loss: 0.2193, Accuracy: 0.9000\n",
      "Epoch [159/1000], Loss: 0.4385, Accuracy: 0.8000\n",
      "Epoch [160/1000], Loss: 0.4085, Accuracy: 0.7500\n",
      "Epoch [161/1000], Loss: 0.3904, Accuracy: 0.8000\n",
      "Epoch [162/1000], Loss: 0.3475, Accuracy: 0.9000\n",
      "Epoch [163/1000], Loss: 0.4318, Accuracy: 0.8000\n",
      "Epoch [164/1000], Loss: 0.4022, Accuracy: 0.8000\n",
      "Epoch [165/1000], Loss: 0.3121, Accuracy: 0.8000\n",
      "Epoch [166/1000], Loss: 0.3182, Accuracy: 0.9000\n",
      "Epoch [167/1000], Loss: 0.4267, Accuracy: 0.9000\n",
      "Epoch [168/1000], Loss: 0.3507, Accuracy: 0.8500\n",
      "Epoch [169/1000], Loss: 0.3043, Accuracy: 0.9000\n",
      "Epoch [170/1000], Loss: 0.3730, Accuracy: 0.8500\n",
      "Epoch [171/1000], Loss: 0.4626, Accuracy: 0.8000\n",
      "Epoch [172/1000], Loss: 0.5086, Accuracy: 0.7500\n",
      "Epoch [173/1000], Loss: 0.3388, Accuracy: 0.8500\n",
      "Epoch [174/1000], Loss: 0.3120, Accuracy: 0.9000\n",
      "Epoch [175/1000], Loss: 0.3030, Accuracy: 0.9000\n",
      "Epoch [176/1000], Loss: 0.5049, Accuracy: 0.7500\n",
      "Epoch [177/1000], Loss: 0.4049, Accuracy: 0.8000\n",
      "Epoch [178/1000], Loss: 0.3406, Accuracy: 0.8500\n",
      "Epoch [179/1000], Loss: 0.2195, Accuracy: 0.9000\n",
      "Epoch [180/1000], Loss: 0.2967, Accuracy: 0.9000\n",
      "Epoch [181/1000], Loss: 0.3528, Accuracy: 0.8500\n",
      "Epoch [182/1000], Loss: 0.3548, Accuracy: 0.7500\n",
      "Epoch [183/1000], Loss: 0.2977, Accuracy: 0.8500\n",
      "Epoch [184/1000], Loss: 0.5045, Accuracy: 0.7500\n",
      "Epoch [185/1000], Loss: 0.3420, Accuracy: 0.8000\n",
      "Epoch [186/1000], Loss: 0.2086, Accuracy: 0.9500\n",
      "Epoch [187/1000], Loss: 0.4511, Accuracy: 0.8000\n",
      "Epoch [188/1000], Loss: 0.4311, Accuracy: 0.8000\n",
      "Epoch [189/1000], Loss: 0.3764, Accuracy: 0.8500\n",
      "Epoch [190/1000], Loss: 0.3712, Accuracy: 0.9000\n",
      "Epoch [191/1000], Loss: 0.2526, Accuracy: 0.9000\n",
      "Epoch [192/1000], Loss: 0.2162, Accuracy: 0.9500\n",
      "Epoch [193/1000], Loss: 0.3285, Accuracy: 0.9000\n",
      "Epoch [194/1000], Loss: 0.8908, Accuracy: 0.6000\n",
      "Epoch [195/1000], Loss: 0.3639, Accuracy: 0.8500\n",
      "Epoch [196/1000], Loss: 0.1887, Accuracy: 1.0000\n",
      "Epoch [197/1000], Loss: 0.4913, Accuracy: 0.8000\n",
      "Epoch [198/1000], Loss: 0.2260, Accuracy: 0.9000\n",
      "Epoch [199/1000], Loss: 0.3096, Accuracy: 0.8500\n",
      "Epoch [200/1000], Loss: 0.6362, Accuracy: 0.8000\n",
      "Epoch [201/1000], Loss: 0.4336, Accuracy: 0.8000\n",
      "Epoch [202/1000], Loss: 0.4830, Accuracy: 0.8500\n",
      "Epoch [203/1000], Loss: 0.2386, Accuracy: 0.9000\n",
      "Epoch [204/1000], Loss: 0.4609, Accuracy: 0.7500\n",
      "Epoch [205/1000], Loss: 0.2407, Accuracy: 1.0000\n",
      "Epoch [206/1000], Loss: 0.3145, Accuracy: 0.9500\n",
      "Epoch [207/1000], Loss: 0.2435, Accuracy: 0.9000\n",
      "Epoch [208/1000], Loss: 0.2897, Accuracy: 0.9000\n",
      "Epoch [209/1000], Loss: 0.5929, Accuracy: 0.7500\n",
      "Epoch [210/1000], Loss: 0.3715, Accuracy: 0.9000\n",
      "Epoch [211/1000], Loss: 0.2718, Accuracy: 0.8500\n",
      "Epoch [212/1000], Loss: 0.3308, Accuracy: 0.8500\n",
      "Epoch [213/1000], Loss: 0.1505, Accuracy: 0.9500\n",
      "Epoch [214/1000], Loss: 0.2035, Accuracy: 0.9500\n",
      "Epoch [215/1000], Loss: 0.6955, Accuracy: 0.6500\n",
      "Epoch [216/1000], Loss: 0.3058, Accuracy: 0.9000\n",
      "Epoch [217/1000], Loss: 0.4640, Accuracy: 0.8500\n",
      "Epoch [218/1000], Loss: 0.4037, Accuracy: 0.8000\n",
      "Epoch [219/1000], Loss: 0.2091, Accuracy: 0.9500\n",
      "Epoch [220/1000], Loss: 0.2808, Accuracy: 0.9000\n",
      "Epoch [221/1000], Loss: 0.3996, Accuracy: 0.8000\n",
      "Epoch [222/1000], Loss: 0.3830, Accuracy: 0.8500\n",
      "Epoch [223/1000], Loss: 0.3112, Accuracy: 0.8500\n",
      "Epoch [224/1000], Loss: 0.3458, Accuracy: 0.8500\n",
      "Epoch [225/1000], Loss: 0.3186, Accuracy: 0.8500\n",
      "Epoch [226/1000], Loss: 0.1500, Accuracy: 1.0000\n",
      "Epoch [227/1000], Loss: 0.3008, Accuracy: 0.8000\n",
      "Epoch [228/1000], Loss: 0.4682, Accuracy: 0.8500\n",
      "Epoch [229/1000], Loss: 0.4455, Accuracy: 0.9000\n",
      "Epoch [230/1000], Loss: 0.5356, Accuracy: 0.8500\n",
      "Epoch [231/1000], Loss: 0.3662, Accuracy: 0.8500\n",
      "Epoch [232/1000], Loss: 0.1703, Accuracy: 1.0000\n",
      "Epoch [233/1000], Loss: 0.3767, Accuracy: 0.8000\n",
      "Epoch [234/1000], Loss: 0.3425, Accuracy: 0.8500\n",
      "Epoch [235/1000], Loss: 0.2250, Accuracy: 0.9000\n",
      "Epoch [236/1000], Loss: 0.2889, Accuracy: 0.9000\n",
      "Epoch [237/1000], Loss: 0.2449, Accuracy: 0.8500\n",
      "Epoch [238/1000], Loss: 0.2788, Accuracy: 0.9000\n",
      "Epoch [239/1000], Loss: 0.3112, Accuracy: 0.8000\n",
      "Epoch [240/1000], Loss: 0.3364, Accuracy: 0.9000\n",
      "Epoch [241/1000], Loss: 0.2029, Accuracy: 0.9000\n",
      "Epoch [242/1000], Loss: 0.1245, Accuracy: 1.0000\n",
      "Epoch [243/1000], Loss: 0.3169, Accuracy: 0.8500\n",
      "Epoch [244/1000], Loss: 0.2416, Accuracy: 0.8500\n",
      "Epoch [245/1000], Loss: 0.2830, Accuracy: 0.9000\n",
      "Epoch [246/1000], Loss: 0.1678, Accuracy: 0.9500\n",
      "Epoch [247/1000], Loss: 0.2739, Accuracy: 0.9000\n",
      "Epoch [248/1000], Loss: 0.4335, Accuracy: 0.7500\n",
      "Epoch [249/1000], Loss: 0.3610, Accuracy: 0.9000\n",
      "Epoch [250/1000], Loss: 0.1678, Accuracy: 0.9500\n",
      "Epoch [251/1000], Loss: 0.1612, Accuracy: 0.9000\n",
      "Epoch [252/1000], Loss: 0.2593, Accuracy: 0.9000\n",
      "Epoch [253/1000], Loss: 0.1706, Accuracy: 1.0000\n",
      "Epoch [254/1000], Loss: 0.2646, Accuracy: 0.8500\n",
      "Epoch [255/1000], Loss: 0.1684, Accuracy: 0.9000\n",
      "Epoch [256/1000], Loss: 0.2469, Accuracy: 0.9000\n",
      "Epoch [257/1000], Loss: 0.0767, Accuracy: 1.0000\n",
      "Epoch [258/1000], Loss: 0.2099, Accuracy: 0.9000\n",
      "Epoch [259/1000], Loss: 0.3860, Accuracy: 0.8500\n",
      "Epoch [260/1000], Loss: 0.3513, Accuracy: 0.8500\n",
      "Epoch [261/1000], Loss: 0.4589, Accuracy: 0.8000\n",
      "Epoch [262/1000], Loss: 0.4242, Accuracy: 0.9000\n",
      "Epoch [263/1000], Loss: 0.2889, Accuracy: 0.9500\n",
      "Epoch [264/1000], Loss: 0.2645, Accuracy: 0.8500\n",
      "Epoch [265/1000], Loss: 0.3965, Accuracy: 0.7000\n",
      "Epoch [266/1000], Loss: 0.6305, Accuracy: 0.7000\n",
      "Epoch [267/1000], Loss: 0.1932, Accuracy: 0.9500\n",
      "Epoch [268/1000], Loss: 0.3685, Accuracy: 0.8500\n",
      "Epoch [269/1000], Loss: 0.3859, Accuracy: 0.8000\n",
      "Epoch [270/1000], Loss: 0.1602, Accuracy: 1.0000\n",
      "Epoch [271/1000], Loss: 0.3024, Accuracy: 0.8500\n",
      "Epoch [272/1000], Loss: 0.2780, Accuracy: 0.8500\n",
      "Epoch [273/1000], Loss: 0.2975, Accuracy: 0.8500\n",
      "Epoch [274/1000], Loss: 0.2265, Accuracy: 0.9000\n",
      "Epoch [275/1000], Loss: 0.2017, Accuracy: 0.9500\n",
      "Epoch [276/1000], Loss: 0.0867, Accuracy: 1.0000\n",
      "Epoch [277/1000], Loss: 0.2501, Accuracy: 0.9000\n",
      "Epoch [278/1000], Loss: 0.1226, Accuracy: 0.9500\n",
      "Epoch [279/1000], Loss: 0.5268, Accuracy: 0.8000\n",
      "Epoch [280/1000], Loss: 0.1007, Accuracy: 1.0000\n",
      "Epoch [281/1000], Loss: 0.2595, Accuracy: 0.8500\n",
      "Epoch [282/1000], Loss: 0.2883, Accuracy: 0.9000\n",
      "Epoch [283/1000], Loss: 0.1244, Accuracy: 0.9500\n",
      "Epoch [284/1000], Loss: 0.0541, Accuracy: 1.0000\n",
      "Epoch [285/1000], Loss: 0.1644, Accuracy: 0.9500\n",
      "Epoch [286/1000], Loss: 0.1177, Accuracy: 1.0000\n",
      "Epoch [287/1000], Loss: 0.4234, Accuracy: 0.9000\n",
      "Epoch [288/1000], Loss: 0.1996, Accuracy: 0.9000\n",
      "Epoch [289/1000], Loss: 0.4070, Accuracy: 0.8500\n",
      "Epoch [290/1000], Loss: 0.5090, Accuracy: 0.7500\n",
      "Epoch [291/1000], Loss: 0.3624, Accuracy: 0.8000\n",
      "Epoch [292/1000], Loss: 0.3523, Accuracy: 0.8500\n",
      "Epoch [293/1000], Loss: 0.2033, Accuracy: 1.0000\n",
      "Epoch [294/1000], Loss: 0.1242, Accuracy: 0.9500\n",
      "Epoch [295/1000], Loss: 0.1487, Accuracy: 0.9500\n",
      "Epoch [296/1000], Loss: 0.2197, Accuracy: 0.9000\n",
      "Epoch [297/1000], Loss: 0.2388, Accuracy: 0.9000\n",
      "Epoch [298/1000], Loss: 0.3229, Accuracy: 0.9000\n",
      "Epoch [299/1000], Loss: 0.2307, Accuracy: 0.9500\n",
      "Epoch [300/1000], Loss: 0.1953, Accuracy: 0.9500\n",
      "Epoch [301/1000], Loss: 0.2512, Accuracy: 0.9500\n",
      "Epoch [302/1000], Loss: 0.3496, Accuracy: 0.9500\n",
      "Epoch [303/1000], Loss: 0.2391, Accuracy: 0.9000\n",
      "Epoch [304/1000], Loss: 0.2534, Accuracy: 0.9000\n",
      "Epoch [305/1000], Loss: 0.1569, Accuracy: 1.0000\n",
      "Epoch [306/1000], Loss: 0.1634, Accuracy: 0.9500\n",
      "Epoch [307/1000], Loss: 0.3557, Accuracy: 0.8000\n",
      "Epoch [308/1000], Loss: 0.1979, Accuracy: 0.9500\n",
      "Epoch [309/1000], Loss: 0.0846, Accuracy: 1.0000\n",
      "Epoch [310/1000], Loss: 0.2030, Accuracy: 0.9500\n",
      "Epoch [311/1000], Loss: 0.1223, Accuracy: 1.0000\n",
      "Epoch [312/1000], Loss: 0.1658, Accuracy: 0.9500\n",
      "Epoch [313/1000], Loss: 0.4482, Accuracy: 0.8000\n",
      "Epoch [314/1000], Loss: 0.2336, Accuracy: 0.9000\n",
      "Epoch [315/1000], Loss: 0.2303, Accuracy: 0.9000\n",
      "Epoch [316/1000], Loss: 0.2390, Accuracy: 0.9000\n",
      "Epoch [317/1000], Loss: 0.1361, Accuracy: 0.9500\n",
      "Epoch [318/1000], Loss: 0.2177, Accuracy: 0.8500\n",
      "Epoch [319/1000], Loss: 0.2127, Accuracy: 0.9500\n",
      "Epoch [320/1000], Loss: 0.0941, Accuracy: 1.0000\n",
      "Epoch [321/1000], Loss: 0.1630, Accuracy: 0.9500\n",
      "Epoch [322/1000], Loss: 0.3983, Accuracy: 0.8500\n",
      "Epoch [323/1000], Loss: 0.1895, Accuracy: 0.9500\n",
      "Epoch [324/1000], Loss: 0.3918, Accuracy: 0.8000\n",
      "Epoch [325/1000], Loss: 0.1524, Accuracy: 1.0000\n",
      "Epoch [326/1000], Loss: 0.0579, Accuracy: 1.0000\n",
      "Epoch [327/1000], Loss: 0.2414, Accuracy: 0.9500\n",
      "Epoch [328/1000], Loss: 0.1692, Accuracy: 0.9500\n",
      "Epoch [329/1000], Loss: 0.3486, Accuracy: 0.9000\n",
      "Epoch [330/1000], Loss: 0.1690, Accuracy: 0.9500\n",
      "Epoch [331/1000], Loss: 0.3674, Accuracy: 0.8000\n",
      "Epoch [332/1000], Loss: 0.2019, Accuracy: 0.8500\n",
      "Epoch [333/1000], Loss: 0.4430, Accuracy: 0.8000\n",
      "Epoch [334/1000], Loss: 0.1332, Accuracy: 1.0000\n",
      "Epoch [335/1000], Loss: 0.3138, Accuracy: 0.9000\n",
      "Epoch [336/1000], Loss: 0.3708, Accuracy: 0.8500\n",
      "Epoch [337/1000], Loss: 0.1890, Accuracy: 0.9500\n",
      "Epoch [338/1000], Loss: 0.2917, Accuracy: 0.8000\n",
      "Epoch [339/1000], Loss: 0.2369, Accuracy: 0.9000\n",
      "Epoch [340/1000], Loss: 0.5443, Accuracy: 0.7500\n",
      "Epoch [341/1000], Loss: 0.2390, Accuracy: 0.9000\n",
      "Epoch [342/1000], Loss: 0.1152, Accuracy: 0.9500\n",
      "Epoch [343/1000], Loss: 0.2008, Accuracy: 0.9000\n",
      "Epoch [344/1000], Loss: 0.1714, Accuracy: 0.9500\n",
      "Epoch [345/1000], Loss: 0.1289, Accuracy: 1.0000\n",
      "Epoch [346/1000], Loss: 0.1180, Accuracy: 1.0000\n",
      "Epoch [347/1000], Loss: 0.0680, Accuracy: 1.0000\n",
      "Epoch [348/1000], Loss: 0.3812, Accuracy: 0.9000\n",
      "Epoch [349/1000], Loss: 0.1845, Accuracy: 0.9500\n",
      "Epoch [350/1000], Loss: 0.1734, Accuracy: 0.9000\n",
      "Epoch [351/1000], Loss: 0.1191, Accuracy: 1.0000\n",
      "Epoch [352/1000], Loss: 0.1952, Accuracy: 0.9000\n",
      "Epoch [353/1000], Loss: 0.2503, Accuracy: 0.9500\n",
      "Epoch [354/1000], Loss: 0.1125, Accuracy: 1.0000\n",
      "Epoch [355/1000], Loss: 0.4126, Accuracy: 0.9000\n",
      "Epoch [356/1000], Loss: 0.1815, Accuracy: 0.9500\n",
      "Epoch [357/1000], Loss: 0.1615, Accuracy: 0.9500\n",
      "Epoch [358/1000], Loss: 0.4788, Accuracy: 0.7500\n",
      "Epoch [359/1000], Loss: 0.1226, Accuracy: 0.9500\n",
      "Epoch [360/1000], Loss: 0.3972, Accuracy: 0.8000\n",
      "Epoch [361/1000], Loss: 0.1293, Accuracy: 0.9500\n",
      "Epoch [362/1000], Loss: 0.1638, Accuracy: 0.9500\n",
      "Epoch [363/1000], Loss: 0.0876, Accuracy: 1.0000\n",
      "Epoch [364/1000], Loss: 0.2783, Accuracy: 0.9500\n",
      "Epoch [365/1000], Loss: 0.2099, Accuracy: 0.9000\n",
      "Epoch [366/1000], Loss: 0.2560, Accuracy: 0.8500\n",
      "Epoch [367/1000], Loss: 0.3589, Accuracy: 0.8000\n",
      "Epoch [368/1000], Loss: 0.3725, Accuracy: 0.8000\n",
      "Epoch [369/1000], Loss: 0.2681, Accuracy: 0.9000\n",
      "Epoch [370/1000], Loss: 0.1135, Accuracy: 0.9500\n",
      "Epoch [371/1000], Loss: 0.1119, Accuracy: 1.0000\n",
      "Epoch [372/1000], Loss: 0.3435, Accuracy: 0.8500\n",
      "Epoch [373/1000], Loss: 0.0846, Accuracy: 1.0000\n",
      "Epoch [374/1000], Loss: 0.2297, Accuracy: 0.8500\n",
      "Epoch [375/1000], Loss: 0.1285, Accuracy: 0.9500\n",
      "Epoch [376/1000], Loss: 0.1262, Accuracy: 1.0000\n",
      "Epoch [377/1000], Loss: 0.1142, Accuracy: 1.0000\n",
      "Epoch [378/1000], Loss: 0.1518, Accuracy: 0.9500\n",
      "Epoch [379/1000], Loss: 0.3322, Accuracy: 0.9500\n",
      "Epoch [380/1000], Loss: 0.1959, Accuracy: 0.9000\n",
      "Epoch [381/1000], Loss: 0.0965, Accuracy: 1.0000\n",
      "Epoch [382/1000], Loss: 0.2687, Accuracy: 0.9000\n",
      "Epoch [383/1000], Loss: 0.3257, Accuracy: 0.8500\n",
      "Epoch [384/1000], Loss: 0.3488, Accuracy: 0.9000\n",
      "Epoch [385/1000], Loss: 0.2671, Accuracy: 0.9000\n",
      "Epoch [386/1000], Loss: 0.1564, Accuracy: 1.0000\n",
      "Epoch [387/1000], Loss: 0.1163, Accuracy: 1.0000\n",
      "Epoch [388/1000], Loss: 0.1569, Accuracy: 0.9500\n",
      "Epoch [389/1000], Loss: 0.1518, Accuracy: 0.9500\n",
      "Epoch [390/1000], Loss: 0.1388, Accuracy: 0.9500\n",
      "Epoch [391/1000], Loss: 0.0583, Accuracy: 1.0000\n",
      "Epoch [392/1000], Loss: 0.1524, Accuracy: 1.0000\n",
      "Epoch [393/1000], Loss: 0.2129, Accuracy: 0.9000\n",
      "Epoch [394/1000], Loss: 0.1015, Accuracy: 1.0000\n",
      "Epoch [395/1000], Loss: 0.2018, Accuracy: 0.9500\n",
      "Epoch [396/1000], Loss: 0.1902, Accuracy: 0.9000\n",
      "Epoch [397/1000], Loss: 0.2444, Accuracy: 0.9000\n",
      "Epoch [398/1000], Loss: 0.3475, Accuracy: 0.9000\n",
      "Epoch [399/1000], Loss: 0.1769, Accuracy: 1.0000\n",
      "Epoch [400/1000], Loss: 0.1809, Accuracy: 0.9500\n",
      "Epoch [401/1000], Loss: 0.2188, Accuracy: 0.9000\n",
      "Epoch [402/1000], Loss: 0.2004, Accuracy: 0.8500\n",
      "Epoch [403/1000], Loss: 0.1172, Accuracy: 0.9500\n",
      "Epoch [404/1000], Loss: 0.1675, Accuracy: 0.9500\n",
      "Epoch [405/1000], Loss: 0.1466, Accuracy: 0.9000\n",
      "Epoch [406/1000], Loss: 0.1314, Accuracy: 0.9500\n",
      "Epoch [407/1000], Loss: 0.1596, Accuracy: 0.9000\n",
      "Epoch [408/1000], Loss: 0.3135, Accuracy: 0.9000\n",
      "Epoch [409/1000], Loss: 0.0996, Accuracy: 1.0000\n",
      "Epoch [410/1000], Loss: 0.3718, Accuracy: 0.8500\n",
      "Epoch [411/1000], Loss: 0.3190, Accuracy: 0.9000\n",
      "Epoch [412/1000], Loss: 0.1350, Accuracy: 0.9500\n",
      "Epoch [413/1000], Loss: 0.0633, Accuracy: 1.0000\n",
      "Epoch [414/1000], Loss: 0.1760, Accuracy: 0.9500\n",
      "Epoch [415/1000], Loss: 0.0835, Accuracy: 1.0000\n",
      "Epoch [416/1000], Loss: 0.1812, Accuracy: 0.9500\n",
      "Epoch [417/1000], Loss: 0.1532, Accuracy: 0.9500\n",
      "Epoch [418/1000], Loss: 0.2394, Accuracy: 0.9000\n",
      "Epoch [419/1000], Loss: 0.4142, Accuracy: 0.9000\n",
      "Epoch [420/1000], Loss: 0.1848, Accuracy: 0.9500\n",
      "Epoch [421/1000], Loss: 0.1457, Accuracy: 1.0000\n",
      "Epoch [422/1000], Loss: 0.1259, Accuracy: 0.9500\n",
      "Epoch [423/1000], Loss: 0.2972, Accuracy: 0.9000\n",
      "Epoch [424/1000], Loss: 0.0761, Accuracy: 0.9500\n",
      "Epoch [425/1000], Loss: 0.2959, Accuracy: 0.8500\n",
      "Epoch [426/1000], Loss: 0.0911, Accuracy: 1.0000\n",
      "Epoch [427/1000], Loss: 0.1998, Accuracy: 0.9500\n",
      "Epoch [428/1000], Loss: 0.2814, Accuracy: 0.8500\n",
      "Epoch [429/1000], Loss: 0.3108, Accuracy: 0.8500\n",
      "Epoch [430/1000], Loss: 0.1893, Accuracy: 0.9000\n",
      "Epoch [431/1000], Loss: 0.0415, Accuracy: 1.0000\n",
      "Epoch [432/1000], Loss: 0.0772, Accuracy: 1.0000\n",
      "Epoch [433/1000], Loss: 0.0529, Accuracy: 1.0000\n",
      "Epoch [434/1000], Loss: 0.2158, Accuracy: 0.9000\n",
      "Epoch [435/1000], Loss: 0.2274, Accuracy: 0.9000\n",
      "Epoch [436/1000], Loss: 0.2194, Accuracy: 0.9000\n",
      "Epoch [437/1000], Loss: 0.0816, Accuracy: 1.0000\n",
      "Epoch [438/1000], Loss: 0.0774, Accuracy: 1.0000\n",
      "Epoch [439/1000], Loss: 0.1462, Accuracy: 0.9500\n",
      "Epoch [440/1000], Loss: 0.0839, Accuracy: 0.9500\n",
      "Epoch [441/1000], Loss: 0.0522, Accuracy: 1.0000\n",
      "Epoch [442/1000], Loss: 0.1249, Accuracy: 0.9500\n",
      "Epoch [443/1000], Loss: 0.1083, Accuracy: 0.9500\n",
      "Epoch [444/1000], Loss: 0.2223, Accuracy: 0.9500\n",
      "Epoch [445/1000], Loss: 0.0567, Accuracy: 1.0000\n",
      "Epoch [446/1000], Loss: 0.1841, Accuracy: 0.9000\n",
      "Epoch [447/1000], Loss: 0.1154, Accuracy: 1.0000\n",
      "Epoch [448/1000], Loss: 0.3357, Accuracy: 0.8000\n",
      "Epoch [449/1000], Loss: 0.1275, Accuracy: 1.0000\n",
      "Epoch [450/1000], Loss: 0.0952, Accuracy: 1.0000\n",
      "Epoch [451/1000], Loss: 0.1713, Accuracy: 0.9500\n",
      "Epoch [452/1000], Loss: 0.1017, Accuracy: 1.0000\n",
      "Epoch [453/1000], Loss: 0.4820, Accuracy: 0.8500\n",
      "Epoch [454/1000], Loss: 0.1063, Accuracy: 0.9000\n",
      "Epoch [455/1000], Loss: 0.0714, Accuracy: 1.0000\n",
      "Epoch [456/1000], Loss: 0.1964, Accuracy: 0.9500\n",
      "Epoch [457/1000], Loss: 0.1276, Accuracy: 0.9500\n",
      "Epoch [458/1000], Loss: 0.0562, Accuracy: 1.0000\n",
      "Epoch [459/1000], Loss: 0.1516, Accuracy: 0.9500\n",
      "Epoch [460/1000], Loss: 0.1019, Accuracy: 0.9500\n",
      "Epoch [461/1000], Loss: 0.0680, Accuracy: 1.0000\n",
      "Epoch [462/1000], Loss: 0.1317, Accuracy: 0.9500\n",
      "Epoch [463/1000], Loss: 0.1139, Accuracy: 1.0000\n",
      "Epoch [464/1000], Loss: 0.1968, Accuracy: 0.9000\n",
      "Epoch [465/1000], Loss: 0.1397, Accuracy: 0.9000\n",
      "Epoch [466/1000], Loss: 0.0935, Accuracy: 0.9500\n",
      "Epoch [467/1000], Loss: 0.0621, Accuracy: 1.0000\n",
      "Epoch [468/1000], Loss: 0.0728, Accuracy: 1.0000\n",
      "Epoch [469/1000], Loss: 0.2527, Accuracy: 0.9000\n",
      "Epoch [470/1000], Loss: 0.1419, Accuracy: 0.9500\n",
      "Epoch [471/1000], Loss: 0.2861, Accuracy: 0.9000\n",
      "Epoch [472/1000], Loss: 0.2047, Accuracy: 0.9500\n",
      "Epoch [473/1000], Loss: 0.2263, Accuracy: 0.9000\n",
      "Epoch [474/1000], Loss: 0.1449, Accuracy: 0.9000\n",
      "Epoch [475/1000], Loss: 0.0826, Accuracy: 0.9500\n",
      "Epoch [476/1000], Loss: 0.0461, Accuracy: 1.0000\n",
      "Epoch [477/1000], Loss: 0.2327, Accuracy: 0.9000\n",
      "Epoch [478/1000], Loss: 0.1189, Accuracy: 0.9500\n",
      "Epoch [479/1000], Loss: 0.1368, Accuracy: 0.9500\n",
      "Epoch [480/1000], Loss: 0.1752, Accuracy: 1.0000\n",
      "Epoch [481/1000], Loss: 0.2316, Accuracy: 0.9000\n",
      "Epoch [482/1000], Loss: 0.1125, Accuracy: 1.0000\n",
      "Epoch [483/1000], Loss: 0.3160, Accuracy: 0.8500\n",
      "Epoch [484/1000], Loss: 0.1291, Accuracy: 0.9500\n",
      "Epoch [485/1000], Loss: 0.0459, Accuracy: 1.0000\n",
      "Epoch [486/1000], Loss: 0.1252, Accuracy: 0.9000\n",
      "Epoch [487/1000], Loss: 0.2733, Accuracy: 0.9000\n",
      "Epoch [488/1000], Loss: 0.2195, Accuracy: 0.9000\n",
      "Epoch [489/1000], Loss: 0.1280, Accuracy: 0.9500\n",
      "Epoch [490/1000], Loss: 0.2857, Accuracy: 0.9500\n",
      "Epoch [491/1000], Loss: 0.1294, Accuracy: 0.9000\n",
      "Epoch [492/1000], Loss: 0.1343, Accuracy: 0.9500\n",
      "Epoch [493/1000], Loss: 0.2015, Accuracy: 0.9500\n",
      "Epoch [494/1000], Loss: 0.1255, Accuracy: 1.0000\n",
      "Epoch [495/1000], Loss: 0.4225, Accuracy: 0.8000\n",
      "Epoch [496/1000], Loss: 0.2165, Accuracy: 0.9500\n",
      "Epoch [497/1000], Loss: 0.1833, Accuracy: 0.9500\n",
      "Epoch [498/1000], Loss: 0.1243, Accuracy: 0.9500\n",
      "Epoch [499/1000], Loss: 0.1624, Accuracy: 0.9500\n",
      "Epoch [500/1000], Loss: 0.0876, Accuracy: 1.0000\n",
      "Epoch [501/1000], Loss: 0.1600, Accuracy: 0.9500\n",
      "Epoch [502/1000], Loss: 0.1935, Accuracy: 0.8500\n",
      "Epoch [503/1000], Loss: 0.1347, Accuracy: 1.0000\n",
      "Epoch [504/1000], Loss: 0.2352, Accuracy: 0.9000\n",
      "Epoch [505/1000], Loss: 0.1526, Accuracy: 0.9500\n",
      "Epoch [506/1000], Loss: 0.1746, Accuracy: 0.9500\n",
      "Epoch [507/1000], Loss: 0.1282, Accuracy: 0.9500\n",
      "Epoch [508/1000], Loss: 0.0660, Accuracy: 1.0000\n",
      "Epoch [509/1000], Loss: 0.1739, Accuracy: 0.9500\n",
      "Epoch [510/1000], Loss: 0.1820, Accuracy: 0.9000\n",
      "Epoch [511/1000], Loss: 0.0956, Accuracy: 1.0000\n",
      "Epoch [512/1000], Loss: 0.1402, Accuracy: 0.9500\n",
      "Epoch [513/1000], Loss: 0.1167, Accuracy: 1.0000\n",
      "Epoch [514/1000], Loss: 0.2353, Accuracy: 0.9000\n",
      "Epoch [515/1000], Loss: 0.0430, Accuracy: 1.0000\n",
      "Epoch [516/1000], Loss: 0.0675, Accuracy: 1.0000\n",
      "Epoch [517/1000], Loss: 0.1106, Accuracy: 0.9500\n",
      "Epoch [518/1000], Loss: 0.1596, Accuracy: 0.9500\n",
      "Epoch [519/1000], Loss: 0.0540, Accuracy: 1.0000\n",
      "Epoch [520/1000], Loss: 0.1554, Accuracy: 0.9500\n",
      "Epoch [521/1000], Loss: 0.0284, Accuracy: 1.0000\n",
      "Epoch [522/1000], Loss: 0.0332, Accuracy: 1.0000\n",
      "Epoch [523/1000], Loss: 0.0690, Accuracy: 1.0000\n",
      "Epoch [524/1000], Loss: 0.1233, Accuracy: 0.9500\n",
      "Epoch [525/1000], Loss: 0.0998, Accuracy: 1.0000\n",
      "Epoch [526/1000], Loss: 0.0528, Accuracy: 1.0000\n",
      "Epoch [527/1000], Loss: 0.3067, Accuracy: 0.9000\n",
      "Epoch [528/1000], Loss: 0.0255, Accuracy: 1.0000\n",
      "Epoch [529/1000], Loss: 0.2226, Accuracy: 0.9500\n",
      "Epoch [530/1000], Loss: 0.3609, Accuracy: 0.9000\n",
      "Epoch [531/1000], Loss: 0.1127, Accuracy: 0.9500\n",
      "Epoch [532/1000], Loss: 0.3539, Accuracy: 0.8500\n",
      "Epoch [533/1000], Loss: 0.2422, Accuracy: 0.9500\n",
      "Epoch [534/1000], Loss: 0.1259, Accuracy: 1.0000\n",
      "Epoch [535/1000], Loss: 0.0880, Accuracy: 1.0000\n",
      "Epoch [536/1000], Loss: 0.0519, Accuracy: 1.0000\n",
      "Epoch [537/1000], Loss: 0.0641, Accuracy: 1.0000\n",
      "Epoch [538/1000], Loss: 0.2632, Accuracy: 0.9000\n",
      "Epoch [539/1000], Loss: 0.1029, Accuracy: 1.0000\n",
      "Epoch [540/1000], Loss: 0.2480, Accuracy: 0.9500\n",
      "Epoch [541/1000], Loss: 0.1138, Accuracy: 0.9500\n",
      "Epoch [542/1000], Loss: 0.0756, Accuracy: 1.0000\n",
      "Epoch [543/1000], Loss: 0.1572, Accuracy: 0.9500\n",
      "Epoch [544/1000], Loss: 0.1061, Accuracy: 1.0000\n",
      "Epoch [545/1000], Loss: 0.1088, Accuracy: 0.9500\n",
      "Epoch [546/1000], Loss: 0.3347, Accuracy: 0.9000\n",
      "Epoch [547/1000], Loss: 0.0804, Accuracy: 1.0000\n",
      "Epoch [548/1000], Loss: 0.2430, Accuracy: 0.9000\n",
      "Epoch [549/1000], Loss: 0.0520, Accuracy: 1.0000\n",
      "Epoch [550/1000], Loss: 0.2674, Accuracy: 0.9000\n",
      "Epoch [551/1000], Loss: 0.1422, Accuracy: 0.9000\n",
      "Epoch [552/1000], Loss: 0.2540, Accuracy: 0.9000\n",
      "Epoch [553/1000], Loss: 0.5550, Accuracy: 0.7000\n",
      "Epoch [554/1000], Loss: 0.1643, Accuracy: 0.9500\n",
      "Epoch [555/1000], Loss: 0.0973, Accuracy: 1.0000\n",
      "Epoch [556/1000], Loss: 0.2958, Accuracy: 0.8500\n",
      "Epoch [557/1000], Loss: 0.0128, Accuracy: 1.0000\n",
      "Epoch [558/1000], Loss: 0.0694, Accuracy: 1.0000\n",
      "Epoch [559/1000], Loss: 0.0575, Accuracy: 1.0000\n",
      "Epoch [560/1000], Loss: 0.2663, Accuracy: 0.8500\n",
      "Epoch [561/1000], Loss: 0.1069, Accuracy: 1.0000\n",
      "Epoch [562/1000], Loss: 0.2160, Accuracy: 0.9000\n",
      "Epoch [563/1000], Loss: 0.0565, Accuracy: 1.0000\n",
      "Epoch [564/1000], Loss: 0.1652, Accuracy: 0.9000\n",
      "Epoch [565/1000], Loss: 0.2627, Accuracy: 0.9000\n",
      "Epoch [566/1000], Loss: 0.1524, Accuracy: 0.9000\n",
      "Epoch [567/1000], Loss: 0.1245, Accuracy: 0.9500\n",
      "Epoch [568/1000], Loss: 0.1050, Accuracy: 1.0000\n",
      "Epoch [569/1000], Loss: 0.1919, Accuracy: 0.9000\n",
      "Epoch [570/1000], Loss: 0.2129, Accuracy: 0.9000\n",
      "Epoch [571/1000], Loss: 0.1368, Accuracy: 0.9500\n",
      "Epoch [572/1000], Loss: 0.0459, Accuracy: 1.0000\n",
      "Epoch [573/1000], Loss: 0.2239, Accuracy: 0.9000\n",
      "Epoch [574/1000], Loss: 0.1098, Accuracy: 0.9000\n",
      "Epoch [575/1000], Loss: 0.2170, Accuracy: 0.9000\n",
      "Epoch [576/1000], Loss: 0.2448, Accuracy: 0.9000\n",
      "Epoch [577/1000], Loss: 0.0289, Accuracy: 1.0000\n",
      "Epoch [578/1000], Loss: 0.3139, Accuracy: 0.9500\n",
      "Epoch [579/1000], Loss: 0.1743, Accuracy: 0.9000\n",
      "Epoch [580/1000], Loss: 0.1516, Accuracy: 1.0000\n",
      "Epoch [581/1000], Loss: 0.0728, Accuracy: 1.0000\n",
      "Epoch [582/1000], Loss: 0.1772, Accuracy: 0.9000\n",
      "Epoch [583/1000], Loss: 0.3282, Accuracy: 0.9500\n",
      "Epoch [584/1000], Loss: 0.2100, Accuracy: 0.9000\n",
      "Epoch [585/1000], Loss: 0.0311, Accuracy: 1.0000\n",
      "Epoch [586/1000], Loss: 0.0580, Accuracy: 1.0000\n",
      "Epoch [587/1000], Loss: 0.0133, Accuracy: 1.0000\n",
      "Epoch [588/1000], Loss: 0.0561, Accuracy: 1.0000\n",
      "Epoch [589/1000], Loss: 0.0265, Accuracy: 1.0000\n",
      "Epoch [590/1000], Loss: 0.0816, Accuracy: 1.0000\n",
      "Epoch [591/1000], Loss: 0.1008, Accuracy: 0.9500\n",
      "Epoch [592/1000], Loss: 0.2276, Accuracy: 0.9000\n",
      "Epoch [593/1000], Loss: 0.3236, Accuracy: 0.8500\n",
      "Epoch [594/1000], Loss: 0.0719, Accuracy: 0.9500\n",
      "Epoch [595/1000], Loss: 0.2044, Accuracy: 0.9000\n",
      "Epoch [596/1000], Loss: 0.1454, Accuracy: 1.0000\n",
      "Epoch [597/1000], Loss: 0.0157, Accuracy: 1.0000\n",
      "Epoch [598/1000], Loss: 0.2507, Accuracy: 0.9500\n",
      "Epoch [599/1000], Loss: 0.1816, Accuracy: 0.9500\n",
      "Epoch [600/1000], Loss: 0.1391, Accuracy: 0.9500\n",
      "Epoch [601/1000], Loss: 0.0826, Accuracy: 0.9500\n",
      "Epoch [602/1000], Loss: 0.1493, Accuracy: 0.9500\n",
      "Epoch [603/1000], Loss: 0.1344, Accuracy: 0.9500\n",
      "Epoch [604/1000], Loss: 0.0908, Accuracy: 0.9500\n",
      "Epoch [605/1000], Loss: 0.0178, Accuracy: 1.0000\n",
      "Epoch [606/1000], Loss: 0.0822, Accuracy: 1.0000\n",
      "Epoch [607/1000], Loss: 0.0110, Accuracy: 1.0000\n",
      "Epoch [608/1000], Loss: 0.0897, Accuracy: 0.9500\n",
      "Epoch [609/1000], Loss: 0.1818, Accuracy: 0.9000\n",
      "Epoch [610/1000], Loss: 0.0161, Accuracy: 1.0000\n",
      "Epoch [611/1000], Loss: 0.1702, Accuracy: 0.9000\n",
      "Epoch [612/1000], Loss: 0.0526, Accuracy: 1.0000\n",
      "Epoch [613/1000], Loss: 0.0304, Accuracy: 1.0000\n",
      "Epoch [614/1000], Loss: 0.0965, Accuracy: 0.9500\n",
      "Epoch [615/1000], Loss: 0.2371, Accuracy: 0.9500\n",
      "Epoch [616/1000], Loss: 0.1781, Accuracy: 0.9500\n",
      "Epoch [617/1000], Loss: 0.0789, Accuracy: 1.0000\n",
      "Epoch [618/1000], Loss: 0.0879, Accuracy: 0.9500\n",
      "Epoch [619/1000], Loss: 0.1181, Accuracy: 0.9500\n",
      "Epoch [620/1000], Loss: 0.0396, Accuracy: 1.0000\n",
      "Epoch [621/1000], Loss: 0.1199, Accuracy: 0.9000\n",
      "Epoch [622/1000], Loss: 0.0612, Accuracy: 1.0000\n",
      "Epoch [623/1000], Loss: 0.0592, Accuracy: 1.0000\n",
      "Epoch [624/1000], Loss: 0.0232, Accuracy: 1.0000\n",
      "Epoch [625/1000], Loss: 0.1104, Accuracy: 0.9500\n",
      "Epoch [626/1000], Loss: 0.2283, Accuracy: 0.8500\n",
      "Epoch [627/1000], Loss: 0.1923, Accuracy: 0.9500\n",
      "Epoch [628/1000], Loss: 0.3043, Accuracy: 0.8500\n",
      "Epoch [629/1000], Loss: 0.0336, Accuracy: 1.0000\n",
      "Epoch [630/1000], Loss: 0.0259, Accuracy: 1.0000\n",
      "Epoch [631/1000], Loss: 0.1402, Accuracy: 0.9000\n",
      "Epoch [632/1000], Loss: 0.1538, Accuracy: 0.9500\n",
      "Epoch [633/1000], Loss: 0.0754, Accuracy: 1.0000\n",
      "Epoch [634/1000], Loss: 0.1325, Accuracy: 0.9000\n",
      "Epoch [635/1000], Loss: 0.1325, Accuracy: 0.9500\n",
      "Epoch [636/1000], Loss: 0.1147, Accuracy: 0.9500\n",
      "Epoch [637/1000], Loss: 0.0634, Accuracy: 1.0000\n",
      "Epoch [638/1000], Loss: 0.2525, Accuracy: 0.8500\n",
      "Epoch [639/1000], Loss: 0.1583, Accuracy: 0.9500\n",
      "Epoch [640/1000], Loss: 0.1609, Accuracy: 0.9000\n",
      "Epoch [641/1000], Loss: 0.0877, Accuracy: 1.0000\n",
      "Epoch [642/1000], Loss: 0.0407, Accuracy: 1.0000\n",
      "Epoch [643/1000], Loss: 0.0114, Accuracy: 1.0000\n",
      "Epoch [644/1000], Loss: 0.0371, Accuracy: 1.0000\n",
      "Epoch [645/1000], Loss: 0.0586, Accuracy: 1.0000\n",
      "Epoch [646/1000], Loss: 0.0898, Accuracy: 1.0000\n",
      "Epoch [647/1000], Loss: 0.1906, Accuracy: 0.9500\n",
      "Epoch [648/1000], Loss: 0.0311, Accuracy: 1.0000\n",
      "Epoch [649/1000], Loss: 0.3668, Accuracy: 0.9000\n",
      "Epoch [650/1000], Loss: 0.2339, Accuracy: 0.9000\n",
      "Epoch [651/1000], Loss: 0.0507, Accuracy: 1.0000\n",
      "Epoch [652/1000], Loss: 0.0588, Accuracy: 1.0000\n",
      "Epoch [653/1000], Loss: 0.0300, Accuracy: 1.0000\n",
      "Epoch [654/1000], Loss: 0.0502, Accuracy: 1.0000\n",
      "Epoch [655/1000], Loss: 0.0564, Accuracy: 1.0000\n",
      "Epoch [656/1000], Loss: 0.0234, Accuracy: 1.0000\n",
      "Epoch [657/1000], Loss: 0.0177, Accuracy: 1.0000\n",
      "Epoch [658/1000], Loss: 0.0269, Accuracy: 1.0000\n",
      "Epoch [659/1000], Loss: 0.2491, Accuracy: 0.8500\n",
      "Epoch [660/1000], Loss: 0.0306, Accuracy: 1.0000\n",
      "Epoch [661/1000], Loss: 0.2209, Accuracy: 0.8500\n",
      "Epoch [662/1000], Loss: 0.1579, Accuracy: 0.9500\n",
      "Epoch [663/1000], Loss: 0.0860, Accuracy: 1.0000\n",
      "Epoch [664/1000], Loss: 0.1346, Accuracy: 0.9500\n",
      "Epoch [665/1000], Loss: 0.1212, Accuracy: 0.9000\n",
      "Epoch [666/1000], Loss: 0.0344, Accuracy: 1.0000\n",
      "Epoch [667/1000], Loss: 0.1865, Accuracy: 0.9000\n",
      "Epoch [668/1000], Loss: 0.0753, Accuracy: 0.9500\n",
      "Epoch [669/1000], Loss: 0.0902, Accuracy: 1.0000\n",
      "Epoch [670/1000], Loss: 0.2358, Accuracy: 0.9000\n",
      "Epoch [671/1000], Loss: 0.0740, Accuracy: 0.9500\n",
      "Epoch [672/1000], Loss: 0.0688, Accuracy: 1.0000\n",
      "Epoch [673/1000], Loss: 0.0778, Accuracy: 0.9500\n",
      "Epoch [674/1000], Loss: 0.0272, Accuracy: 1.0000\n",
      "Epoch [675/1000], Loss: 0.1881, Accuracy: 0.9500\n",
      "Epoch [676/1000], Loss: 0.0305, Accuracy: 1.0000\n",
      "Epoch [677/1000], Loss: 0.2448, Accuracy: 0.9000\n",
      "Epoch [678/1000], Loss: 0.0442, Accuracy: 1.0000\n",
      "Epoch [679/1000], Loss: 0.0819, Accuracy: 1.0000\n",
      "Epoch [680/1000], Loss: 0.0781, Accuracy: 0.9500\n",
      "Epoch [681/1000], Loss: 0.1368, Accuracy: 0.9000\n",
      "Epoch [682/1000], Loss: 0.1084, Accuracy: 0.9500\n",
      "Epoch [683/1000], Loss: 0.2236, Accuracy: 0.9500\n",
      "Epoch [684/1000], Loss: 0.1281, Accuracy: 0.9500\n",
      "Epoch [685/1000], Loss: 0.0678, Accuracy: 0.9500\n",
      "Epoch [686/1000], Loss: 0.3078, Accuracy: 0.9000\n",
      "Epoch [687/1000], Loss: 0.4341, Accuracy: 0.8500\n",
      "Epoch [688/1000], Loss: 0.3820, Accuracy: 0.8000\n",
      "Epoch [689/1000], Loss: 0.0191, Accuracy: 1.0000\n",
      "Epoch [690/1000], Loss: 0.0145, Accuracy: 1.0000\n",
      "Epoch [691/1000], Loss: 0.1197, Accuracy: 0.9500\n",
      "Epoch [692/1000], Loss: 0.1685, Accuracy: 0.9500\n",
      "Epoch [693/1000], Loss: 0.1484, Accuracy: 0.9500\n",
      "Epoch [694/1000], Loss: 0.1010, Accuracy: 1.0000\n",
      "Epoch [695/1000], Loss: 0.3592, Accuracy: 0.9500\n",
      "Epoch [696/1000], Loss: 0.0658, Accuracy: 1.0000\n",
      "Epoch [697/1000], Loss: 0.0406, Accuracy: 1.0000\n",
      "Epoch [698/1000], Loss: 0.0488, Accuracy: 1.0000\n",
      "Epoch [699/1000], Loss: 0.0085, Accuracy: 1.0000\n",
      "Epoch [700/1000], Loss: 0.1298, Accuracy: 0.9000\n",
      "Epoch [701/1000], Loss: 0.1050, Accuracy: 0.9500\n",
      "Epoch [702/1000], Loss: 0.0712, Accuracy: 1.0000\n",
      "Epoch [703/1000], Loss: 0.0546, Accuracy: 1.0000\n",
      "Epoch [704/1000], Loss: 0.1950, Accuracy: 0.9000\n",
      "Epoch [705/1000], Loss: 0.0443, Accuracy: 1.0000\n",
      "Epoch [706/1000], Loss: 0.0071, Accuracy: 1.0000\n",
      "Epoch [707/1000], Loss: 0.0476, Accuracy: 1.0000\n",
      "Epoch [708/1000], Loss: 0.0688, Accuracy: 1.0000\n",
      "Epoch [709/1000], Loss: 0.0474, Accuracy: 0.9500\n",
      "Epoch [710/1000], Loss: 0.1161, Accuracy: 0.9500\n",
      "Epoch [711/1000], Loss: 0.0505, Accuracy: 1.0000\n",
      "Epoch [712/1000], Loss: 0.0970, Accuracy: 0.9500\n",
      "Epoch [713/1000], Loss: 0.1285, Accuracy: 0.9000\n",
      "Epoch [714/1000], Loss: 0.1214, Accuracy: 0.9000\n",
      "Epoch [715/1000], Loss: 0.1065, Accuracy: 0.9500\n",
      "Epoch [716/1000], Loss: 0.0410, Accuracy: 1.0000\n",
      "Epoch [717/1000], Loss: 0.2521, Accuracy: 0.9000\n",
      "Epoch [718/1000], Loss: 0.0701, Accuracy: 1.0000\n",
      "Epoch [719/1000], Loss: 0.0659, Accuracy: 1.0000\n",
      "Epoch [720/1000], Loss: 0.0334, Accuracy: 1.0000\n",
      "Epoch [721/1000], Loss: 0.0620, Accuracy: 1.0000\n",
      "Epoch [722/1000], Loss: 0.0223, Accuracy: 1.0000\n",
      "Epoch [723/1000], Loss: 0.0222, Accuracy: 1.0000\n",
      "Epoch [724/1000], Loss: 0.0176, Accuracy: 1.0000\n",
      "Epoch [725/1000], Loss: 0.0505, Accuracy: 0.9500\n",
      "Epoch [726/1000], Loss: 0.0173, Accuracy: 1.0000\n",
      "Epoch [727/1000], Loss: 0.0610, Accuracy: 1.0000\n",
      "Epoch [728/1000], Loss: 0.0260, Accuracy: 1.0000\n",
      "Epoch [729/1000], Loss: 0.1077, Accuracy: 0.9000\n",
      "Epoch [730/1000], Loss: 0.0428, Accuracy: 1.0000\n",
      "Epoch [731/1000], Loss: 0.0213, Accuracy: 1.0000\n",
      "Epoch [732/1000], Loss: 0.0407, Accuracy: 1.0000\n",
      "Epoch [733/1000], Loss: 0.1910, Accuracy: 0.9500\n",
      "Epoch [734/1000], Loss: 0.0301, Accuracy: 1.0000\n",
      "Epoch [735/1000], Loss: 0.1329, Accuracy: 0.9000\n",
      "Epoch [736/1000], Loss: 0.1588, Accuracy: 0.9000\n",
      "Epoch [737/1000], Loss: 0.0482, Accuracy: 1.0000\n",
      "Epoch [738/1000], Loss: 0.1010, Accuracy: 0.9500\n",
      "Epoch [739/1000], Loss: 0.3341, Accuracy: 0.9500\n",
      "Epoch [740/1000], Loss: 0.3811, Accuracy: 0.8500\n",
      "Epoch [741/1000], Loss: 0.0907, Accuracy: 0.9500\n",
      "Epoch [742/1000], Loss: 0.0347, Accuracy: 1.0000\n",
      "Epoch [743/1000], Loss: 0.0690, Accuracy: 0.9500\n",
      "Epoch [744/1000], Loss: 0.0359, Accuracy: 1.0000\n",
      "Epoch [745/1000], Loss: 0.0262, Accuracy: 1.0000\n",
      "Epoch [746/1000], Loss: 0.0348, Accuracy: 1.0000\n",
      "Epoch [747/1000], Loss: 0.1174, Accuracy: 0.9500\n",
      "Epoch [748/1000], Loss: 0.0255, Accuracy: 1.0000\n",
      "Epoch [749/1000], Loss: 0.0594, Accuracy: 1.0000\n",
      "Epoch [750/1000], Loss: 0.0696, Accuracy: 1.0000\n",
      "Epoch [751/1000], Loss: 0.0821, Accuracy: 1.0000\n",
      "Epoch [752/1000], Loss: 0.0299, Accuracy: 1.0000\n",
      "Epoch [753/1000], Loss: 0.0945, Accuracy: 0.9500\n",
      "Epoch [754/1000], Loss: 0.2803, Accuracy: 0.9000\n",
      "Epoch [755/1000], Loss: 0.0102, Accuracy: 1.0000\n",
      "Epoch [756/1000], Loss: 0.0527, Accuracy: 0.9500\n",
      "Epoch [757/1000], Loss: 0.0786, Accuracy: 0.9500\n",
      "Epoch [758/1000], Loss: 0.1917, Accuracy: 0.9500\n",
      "Epoch [759/1000], Loss: 0.1454, Accuracy: 0.9500\n",
      "Epoch [760/1000], Loss: 0.0936, Accuracy: 1.0000\n",
      "Epoch [761/1000], Loss: 0.0209, Accuracy: 1.0000\n",
      "Epoch [762/1000], Loss: 0.0428, Accuracy: 1.0000\n",
      "Epoch [763/1000], Loss: 0.1117, Accuracy: 0.9500\n",
      "Epoch [764/1000], Loss: 0.0202, Accuracy: 1.0000\n",
      "Epoch [765/1000], Loss: 0.0406, Accuracy: 1.0000\n",
      "Epoch [766/1000], Loss: 0.1427, Accuracy: 0.9500\n",
      "Epoch [767/1000], Loss: 0.1652, Accuracy: 0.9000\n",
      "Epoch [768/1000], Loss: 0.0671, Accuracy: 0.9500\n",
      "Epoch [769/1000], Loss: 0.0293, Accuracy: 1.0000\n",
      "Epoch [770/1000], Loss: 0.0254, Accuracy: 1.0000\n",
      "Epoch [771/1000], Loss: 0.0092, Accuracy: 1.0000\n",
      "Epoch [772/1000], Loss: 0.0134, Accuracy: 1.0000\n",
      "Epoch [773/1000], Loss: 0.2872, Accuracy: 0.8500\n",
      "Epoch [774/1000], Loss: 0.0434, Accuracy: 1.0000\n",
      "Epoch [775/1000], Loss: 0.0486, Accuracy: 1.0000\n",
      "Epoch [776/1000], Loss: 0.0151, Accuracy: 1.0000\n",
      "Epoch [777/1000], Loss: 0.0852, Accuracy: 0.9500\n",
      "Epoch [778/1000], Loss: 0.1417, Accuracy: 0.9500\n",
      "Epoch [779/1000], Loss: 0.2488, Accuracy: 0.9000\n",
      "Epoch [780/1000], Loss: 0.0281, Accuracy: 1.0000\n",
      "Epoch [781/1000], Loss: 0.0374, Accuracy: 1.0000\n",
      "Epoch [782/1000], Loss: 0.0651, Accuracy: 0.9500\n",
      "Epoch [783/1000], Loss: 0.0748, Accuracy: 0.9500\n",
      "Epoch [784/1000], Loss: 0.0952, Accuracy: 0.9500\n",
      "Epoch [785/1000], Loss: 0.0729, Accuracy: 1.0000\n",
      "Epoch [786/1000], Loss: 0.0955, Accuracy: 1.0000\n",
      "Epoch [787/1000], Loss: 0.0392, Accuracy: 1.0000\n",
      "Epoch [788/1000], Loss: 0.1245, Accuracy: 0.9500\n",
      "Epoch [789/1000], Loss: 0.0816, Accuracy: 1.0000\n",
      "Epoch [790/1000], Loss: 0.1316, Accuracy: 0.9500\n",
      "Epoch [791/1000], Loss: 0.0785, Accuracy: 1.0000\n",
      "Epoch [792/1000], Loss: 0.2929, Accuracy: 0.9000\n",
      "Epoch [793/1000], Loss: 0.1856, Accuracy: 0.9000\n",
      "Epoch [794/1000], Loss: 0.1731, Accuracy: 0.9500\n",
      "Epoch [795/1000], Loss: 0.1866, Accuracy: 0.9500\n",
      "Epoch [796/1000], Loss: 0.0474, Accuracy: 1.0000\n",
      "Epoch [797/1000], Loss: 0.0658, Accuracy: 1.0000\n",
      "Epoch [798/1000], Loss: 0.0470, Accuracy: 1.0000\n",
      "Epoch [799/1000], Loss: 0.0939, Accuracy: 0.9500\n",
      "Epoch [800/1000], Loss: 0.0223, Accuracy: 1.0000\n",
      "Epoch [801/1000], Loss: 0.1720, Accuracy: 0.8500\n",
      "Epoch [802/1000], Loss: 0.0891, Accuracy: 0.9500\n",
      "Epoch [803/1000], Loss: 0.0461, Accuracy: 1.0000\n",
      "Epoch [804/1000], Loss: 0.1520, Accuracy: 0.9000\n",
      "Epoch [805/1000], Loss: 0.0245, Accuracy: 1.0000\n",
      "Epoch [806/1000], Loss: 0.0128, Accuracy: 1.0000\n",
      "Epoch [807/1000], Loss: 0.0067, Accuracy: 1.0000\n",
      "Epoch [808/1000], Loss: 0.0213, Accuracy: 1.0000\n",
      "Epoch [809/1000], Loss: 0.1794, Accuracy: 0.9000\n",
      "Epoch [810/1000], Loss: 0.0571, Accuracy: 1.0000\n",
      "Epoch [811/1000], Loss: 0.0158, Accuracy: 1.0000\n",
      "Epoch [812/1000], Loss: 0.0264, Accuracy: 1.0000\n",
      "Epoch [813/1000], Loss: 0.1333, Accuracy: 0.9500\n",
      "Epoch [814/1000], Loss: 0.2529, Accuracy: 0.9000\n",
      "Epoch [815/1000], Loss: 0.0727, Accuracy: 0.9500\n",
      "Epoch [816/1000], Loss: 0.0932, Accuracy: 0.9500\n",
      "Epoch [817/1000], Loss: 0.1571, Accuracy: 0.9000\n",
      "Epoch [818/1000], Loss: 0.2465, Accuracy: 0.9000\n",
      "Epoch [819/1000], Loss: 0.2423, Accuracy: 0.9000\n",
      "Epoch [820/1000], Loss: 0.0250, Accuracy: 1.0000\n",
      "Epoch [821/1000], Loss: 0.0160, Accuracy: 1.0000\n",
      "Epoch [822/1000], Loss: 0.0364, Accuracy: 1.0000\n",
      "Epoch [823/1000], Loss: 0.2483, Accuracy: 0.9500\n",
      "Epoch [824/1000], Loss: 0.1334, Accuracy: 0.9000\n",
      "Epoch [825/1000], Loss: 0.0200, Accuracy: 1.0000\n",
      "Epoch [826/1000], Loss: 0.0121, Accuracy: 1.0000\n",
      "Epoch [827/1000], Loss: 0.0413, Accuracy: 1.0000\n",
      "Epoch [828/1000], Loss: 0.0824, Accuracy: 0.9500\n",
      "Epoch [829/1000], Loss: 0.0708, Accuracy: 0.9500\n",
      "Epoch [830/1000], Loss: 0.1650, Accuracy: 0.9000\n",
      "Epoch [831/1000], Loss: 0.1832, Accuracy: 0.9000\n",
      "Epoch [832/1000], Loss: 0.0151, Accuracy: 1.0000\n",
      "Epoch [833/1000], Loss: 0.0102, Accuracy: 1.0000\n",
      "Epoch [834/1000], Loss: 0.0923, Accuracy: 0.9500\n",
      "Epoch [835/1000], Loss: 0.0239, Accuracy: 1.0000\n",
      "Epoch [836/1000], Loss: 0.0343, Accuracy: 1.0000\n",
      "Epoch [837/1000], Loss: 0.0489, Accuracy: 1.0000\n",
      "Epoch [838/1000], Loss: 0.2473, Accuracy: 0.8500\n",
      "Epoch [839/1000], Loss: 0.0476, Accuracy: 1.0000\n",
      "Epoch [840/1000], Loss: 0.0520, Accuracy: 1.0000\n",
      "Epoch [841/1000], Loss: 0.0474, Accuracy: 1.0000\n",
      "Epoch [842/1000], Loss: 0.0798, Accuracy: 0.9500\n",
      "Epoch [843/1000], Loss: 0.1903, Accuracy: 0.9500\n",
      "Epoch [844/1000], Loss: 0.1424, Accuracy: 1.0000\n",
      "Epoch [845/1000], Loss: 0.0352, Accuracy: 1.0000\n",
      "Epoch [846/1000], Loss: 0.0406, Accuracy: 1.0000\n",
      "Epoch [847/1000], Loss: 0.0979, Accuracy: 0.9500\n",
      "Epoch [848/1000], Loss: 0.0841, Accuracy: 1.0000\n",
      "Epoch [849/1000], Loss: 0.0810, Accuracy: 0.9500\n",
      "Epoch [850/1000], Loss: 0.1774, Accuracy: 0.9000\n",
      "Epoch [851/1000], Loss: 0.3133, Accuracy: 0.9000\n",
      "Epoch [852/1000], Loss: 0.0134, Accuracy: 1.0000\n",
      "Epoch [853/1000], Loss: 0.0347, Accuracy: 1.0000\n",
      "Epoch [854/1000], Loss: 0.0050, Accuracy: 1.0000\n",
      "Epoch [855/1000], Loss: 0.0337, Accuracy: 1.0000\n",
      "Epoch [856/1000], Loss: 0.0170, Accuracy: 1.0000\n",
      "Epoch [857/1000], Loss: 0.0243, Accuracy: 1.0000\n",
      "Epoch [858/1000], Loss: 0.1500, Accuracy: 0.9000\n",
      "Epoch [859/1000], Loss: 0.0607, Accuracy: 1.0000\n",
      "Epoch [860/1000], Loss: 0.0100, Accuracy: 1.0000\n",
      "Epoch [861/1000], Loss: 0.0831, Accuracy: 0.9500\n",
      "Epoch [862/1000], Loss: 0.1288, Accuracy: 0.9500\n",
      "Epoch [863/1000], Loss: 0.0068, Accuracy: 1.0000\n",
      "Epoch [864/1000], Loss: 0.0222, Accuracy: 1.0000\n",
      "Epoch [865/1000], Loss: 0.0072, Accuracy: 1.0000\n",
      "Epoch [866/1000], Loss: 0.0832, Accuracy: 0.9500\n",
      "Epoch [867/1000], Loss: 0.0725, Accuracy: 0.9500\n",
      "Epoch [868/1000], Loss: 0.0382, Accuracy: 1.0000\n",
      "Epoch [869/1000], Loss: 0.0618, Accuracy: 1.0000\n",
      "Epoch [870/1000], Loss: 0.0213, Accuracy: 1.0000\n",
      "Epoch [871/1000], Loss: 0.0419, Accuracy: 1.0000\n",
      "Epoch [872/1000], Loss: 0.0104, Accuracy: 1.0000\n",
      "Epoch [873/1000], Loss: 0.1378, Accuracy: 0.9500\n",
      "Epoch [874/1000], Loss: 0.0605, Accuracy: 1.0000\n",
      "Epoch [875/1000], Loss: 0.1916, Accuracy: 0.9000\n",
      "Epoch [876/1000], Loss: 0.0065, Accuracy: 1.0000\n",
      "Epoch [877/1000], Loss: 0.0907, Accuracy: 0.9500\n",
      "Epoch [878/1000], Loss: 0.0638, Accuracy: 0.9500\n",
      "Epoch [879/1000], Loss: 0.0195, Accuracy: 1.0000\n",
      "Epoch [880/1000], Loss: 0.2694, Accuracy: 0.9000\n",
      "Epoch [881/1000], Loss: 0.0221, Accuracy: 1.0000\n",
      "Epoch [882/1000], Loss: 0.0181, Accuracy: 1.0000\n",
      "Epoch [883/1000], Loss: 0.0643, Accuracy: 0.9500\n",
      "Epoch [884/1000], Loss: 0.0267, Accuracy: 1.0000\n",
      "Epoch [885/1000], Loss: 0.0124, Accuracy: 1.0000\n",
      "Epoch [886/1000], Loss: 0.0216, Accuracy: 1.0000\n",
      "Epoch [887/1000], Loss: 0.0993, Accuracy: 0.9500\n",
      "Epoch [888/1000], Loss: 0.0221, Accuracy: 1.0000\n",
      "Epoch [889/1000], Loss: 0.0089, Accuracy: 1.0000\n",
      "Epoch [890/1000], Loss: 0.2377, Accuracy: 0.9000\n",
      "Epoch [891/1000], Loss: 0.0158, Accuracy: 1.0000\n",
      "Epoch [892/1000], Loss: 0.2898, Accuracy: 0.9500\n",
      "Epoch [893/1000], Loss: 0.0133, Accuracy: 1.0000\n",
      "Epoch [894/1000], Loss: 0.0433, Accuracy: 1.0000\n",
      "Epoch [895/1000], Loss: 0.0123, Accuracy: 1.0000\n",
      "Epoch [896/1000], Loss: 0.0291, Accuracy: 1.0000\n",
      "Epoch [897/1000], Loss: 0.0578, Accuracy: 1.0000\n",
      "Epoch [898/1000], Loss: 0.1157, Accuracy: 0.9500\n",
      "Epoch [899/1000], Loss: 0.0889, Accuracy: 0.9500\n",
      "Epoch [900/1000], Loss: 0.0455, Accuracy: 1.0000\n",
      "Epoch [901/1000], Loss: 0.0173, Accuracy: 1.0000\n",
      "Epoch [902/1000], Loss: 0.0518, Accuracy: 0.9500\n",
      "Epoch [903/1000], Loss: 0.1312, Accuracy: 0.9500\n",
      "Epoch [904/1000], Loss: 0.2948, Accuracy: 0.9000\n",
      "Epoch [905/1000], Loss: 0.0532, Accuracy: 1.0000\n",
      "Epoch [906/1000], Loss: 0.0511, Accuracy: 1.0000\n",
      "Epoch [907/1000], Loss: 0.0572, Accuracy: 0.9500\n",
      "Epoch [908/1000], Loss: 0.1081, Accuracy: 1.0000\n",
      "Epoch [909/1000], Loss: 0.0167, Accuracy: 1.0000\n",
      "Epoch [910/1000], Loss: 0.0221, Accuracy: 1.0000\n",
      "Epoch [911/1000], Loss: 0.1015, Accuracy: 0.9500\n",
      "Epoch [912/1000], Loss: 0.0280, Accuracy: 1.0000\n",
      "Epoch [913/1000], Loss: 0.1772, Accuracy: 0.9500\n",
      "Epoch [914/1000], Loss: 0.2354, Accuracy: 0.9000\n",
      "Epoch [915/1000], Loss: 0.0659, Accuracy: 0.9500\n",
      "Epoch [916/1000], Loss: 0.1343, Accuracy: 0.9500\n",
      "Epoch [917/1000], Loss: 0.0205, Accuracy: 1.0000\n",
      "Epoch [918/1000], Loss: 0.0440, Accuracy: 1.0000\n",
      "Epoch [919/1000], Loss: 0.0215, Accuracy: 1.0000\n",
      "Epoch [920/1000], Loss: 0.0737, Accuracy: 0.9500\n",
      "Epoch [921/1000], Loss: 0.0114, Accuracy: 1.0000\n",
      "Epoch [922/1000], Loss: 0.0825, Accuracy: 0.9500\n",
      "Epoch [923/1000], Loss: 0.0129, Accuracy: 1.0000\n",
      "Epoch [924/1000], Loss: 0.0175, Accuracy: 1.0000\n",
      "Epoch [925/1000], Loss: 0.0396, Accuracy: 1.0000\n",
      "Epoch [926/1000], Loss: 0.0143, Accuracy: 1.0000\n",
      "Epoch [927/1000], Loss: 0.0668, Accuracy: 0.9500\n",
      "Epoch [928/1000], Loss: 0.0335, Accuracy: 1.0000\n",
      "Epoch [929/1000], Loss: 0.0719, Accuracy: 0.9500\n",
      "Epoch [930/1000], Loss: 0.0240, Accuracy: 1.0000\n",
      "Epoch [931/1000], Loss: 0.0098, Accuracy: 1.0000\n",
      "Epoch [932/1000], Loss: 0.0832, Accuracy: 0.9500\n",
      "Epoch [933/1000], Loss: 0.0303, Accuracy: 1.0000\n",
      "Epoch [934/1000], Loss: 0.0279, Accuracy: 1.0000\n",
      "Epoch [935/1000], Loss: 0.0625, Accuracy: 0.9500\n",
      "Epoch [936/1000], Loss: 0.0449, Accuracy: 1.0000\n",
      "Epoch [937/1000], Loss: 0.1282, Accuracy: 0.9500\n",
      "Epoch [938/1000], Loss: 0.0165, Accuracy: 1.0000\n",
      "Epoch [939/1000], Loss: 0.0205, Accuracy: 1.0000\n",
      "Epoch [940/1000], Loss: 0.0263, Accuracy: 1.0000\n",
      "Epoch [941/1000], Loss: 0.0049, Accuracy: 1.0000\n",
      "Epoch [942/1000], Loss: 0.1575, Accuracy: 0.9000\n",
      "Epoch [943/1000], Loss: 0.0445, Accuracy: 1.0000\n",
      "Epoch [944/1000], Loss: 0.0661, Accuracy: 0.9500\n",
      "Epoch [945/1000], Loss: 0.1212, Accuracy: 0.9500\n",
      "Epoch [946/1000], Loss: 0.0752, Accuracy: 1.0000\n",
      "Epoch [947/1000], Loss: 0.0960, Accuracy: 0.9000\n",
      "Epoch [948/1000], Loss: 0.0064, Accuracy: 1.0000\n",
      "Epoch [949/1000], Loss: 0.0720, Accuracy: 0.9500\n",
      "Epoch [950/1000], Loss: 0.0072, Accuracy: 1.0000\n",
      "Epoch [951/1000], Loss: 0.0763, Accuracy: 0.9500\n",
      "Epoch [952/1000], Loss: 0.1297, Accuracy: 0.9500\n",
      "Epoch [953/1000], Loss: 0.0201, Accuracy: 1.0000\n",
      "Epoch [954/1000], Loss: 0.0645, Accuracy: 1.0000\n",
      "Epoch [955/1000], Loss: 0.0898, Accuracy: 1.0000\n",
      "Epoch [956/1000], Loss: 0.0762, Accuracy: 1.0000\n",
      "Epoch [957/1000], Loss: 0.0340, Accuracy: 1.0000\n",
      "Epoch [958/1000], Loss: 0.0096, Accuracy: 1.0000\n",
      "Epoch [959/1000], Loss: 0.0302, Accuracy: 1.0000\n",
      "Epoch [960/1000], Loss: 0.0280, Accuracy: 1.0000\n",
      "Epoch [961/1000], Loss: 0.0687, Accuracy: 0.9500\n",
      "Epoch [962/1000], Loss: 0.0159, Accuracy: 1.0000\n",
      "Epoch [963/1000], Loss: 0.0430, Accuracy: 0.9500\n",
      "Epoch [964/1000], Loss: 0.0322, Accuracy: 1.0000\n",
      "Epoch [965/1000], Loss: 0.0368, Accuracy: 1.0000\n",
      "Epoch [966/1000], Loss: 0.0204, Accuracy: 1.0000\n",
      "Epoch [967/1000], Loss: 0.0030, Accuracy: 1.0000\n",
      "Epoch [968/1000], Loss: 0.0540, Accuracy: 1.0000\n",
      "Epoch [969/1000], Loss: 0.2866, Accuracy: 0.9000\n",
      "Epoch [970/1000], Loss: 0.3234, Accuracy: 0.9500\n",
      "Epoch [971/1000], Loss: 0.1065, Accuracy: 0.9500\n",
      "Epoch [972/1000], Loss: 0.0144, Accuracy: 1.0000\n",
      "Epoch [973/1000], Loss: 0.0146, Accuracy: 1.0000\n",
      "Epoch [974/1000], Loss: 0.0424, Accuracy: 1.0000\n",
      "Epoch [975/1000], Loss: 0.0093, Accuracy: 1.0000\n",
      "Epoch [976/1000], Loss: 0.1340, Accuracy: 0.9500\n",
      "Epoch [977/1000], Loss: 0.2443, Accuracy: 0.9500\n",
      "Epoch [978/1000], Loss: 0.0481, Accuracy: 1.0000\n",
      "Epoch [979/1000], Loss: 0.0522, Accuracy: 0.9500\n",
      "Epoch [980/1000], Loss: 0.0539, Accuracy: 1.0000\n",
      "Epoch [981/1000], Loss: 0.0137, Accuracy: 1.0000\n",
      "Epoch [982/1000], Loss: 0.0792, Accuracy: 0.9500\n",
      "Epoch [983/1000], Loss: 0.0274, Accuracy: 1.0000\n",
      "Epoch [984/1000], Loss: 0.0136, Accuracy: 1.0000\n",
      "Epoch [985/1000], Loss: 0.1472, Accuracy: 0.9500\n",
      "Epoch [986/1000], Loss: 0.0384, Accuracy: 1.0000\n",
      "Epoch [987/1000], Loss: 0.0262, Accuracy: 1.0000\n",
      "Epoch [988/1000], Loss: 0.0384, Accuracy: 1.0000\n",
      "Epoch [989/1000], Loss: 0.0386, Accuracy: 1.0000\n",
      "Epoch [990/1000], Loss: 0.1435, Accuracy: 0.9500\n",
      "Epoch [991/1000], Loss: 0.0260, Accuracy: 1.0000\n",
      "Epoch [992/1000], Loss: 0.0160, Accuracy: 1.0000\n",
      "Epoch [993/1000], Loss: 0.0431, Accuracy: 1.0000\n",
      "Epoch [994/1000], Loss: 0.0723, Accuracy: 0.9500\n",
      "Epoch [995/1000], Loss: 0.0542, Accuracy: 1.0000\n",
      "Epoch [996/1000], Loss: 0.1178, Accuracy: 0.9500\n",
      "Epoch [997/1000], Loss: 0.0046, Accuracy: 1.0000\n",
      "Epoch [998/1000], Loss: 0.0063, Accuracy: 1.0000\n",
      "Epoch [999/1000], Loss: 0.0179, Accuracy: 1.0000\n",
      "Epoch [1000/1000], Loss: 0.0099, Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(tfnetwork.parameters(), lr = 0.001)\n",
    "\n",
    "tfnetwork.train()\n",
    "\n",
    "# print(i)\n",
    "# for train_features, train_labels in train_dl:\n",
    "#     i += 1\n",
    "#     print(i)\n",
    "n_epochs = 1000\n",
    "    \n",
    "for epoch in range(n_epochs):\n",
    "    for train_features, train_labels in train_dl:\n",
    "        \n",
    "        # Forward pass\n",
    "        train_out = tfnetwork(train_features)\n",
    "        loss = loss_fn(train_out, train_labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    # print('train_out.shape', train_out.shape)\n",
    "    accuracy = get_accuracy(train_out, train_labels)\n",
    "    \n",
    "    tfnetwork.eval()\n",
    "    test_features, test_labels = \n",
    "    print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "2b32d268-f2d2-4a29-bae5-14c9f8a07232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerClassifier(\n",
       "  (positional_layer): Linear(in_features=1, out_features=36, bias=True)\n",
       "  (embed_layer): Linear(in_features=4, out_features=36, bias=True)\n",
       "  (encoderlayer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=36, out_features=100, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=100, out_features=36, bias=True)\n",
       "    (norm1): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=36, out_features=100, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=100, out_features=36, bias=True)\n",
       "        (norm1): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=36, out_features=100, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=100, out_features=36, bias=True)\n",
       "        (norm1): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=36, out_features=36, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=36, out_features=100, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=100, out_features=36, bias=True)\n",
       "        (norm1): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (class_encoder): Linear(in_features=36, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfnetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "96f781e1-74da-46fc-a9bc-717cd19f2ffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 36])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfnetwork_out = tfnetwork(tf_test)\n",
    "torch.max(tfnetwork_out,dim = 1)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "785e5e8d-7579-4edf-8597-ada7443140e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-inf, -inf, -inf, -inf],\n",
       "        [0., -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf],\n",
       "        [0., 0., 0., -inf]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.triu(torch.ones(4, 4) * float('-inf'), diagonal=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "afcaad30-981b-4a75-a214-de491bf2757c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac10e44-7ccd-4f4e-ab48-255648980338",
   "metadata": {},
   "source": [
    "## Old S2 pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8801cb4f-7504-4665-bd51-91a431e0d23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class s2Dataset(Dataset):\n",
    "#     \"\"\"Sentinel 2 dataset\"\"\"\n",
    "    \n",
    "#     def __init__(self, proj_path, class_colname):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             proj_path (string): path to manclassify project\n",
    "#         \"\"\"\n",
    "#         self.proj_path = proj_path\n",
    "#         proj_normpath = os.path.normpath(proj_path)\n",
    "#         proj_dirname = proj_normpath.split(os.sep)[-1]\n",
    "#         self.proj_name = re.sub(\"_classification$\",\"\",proj_dirname)\n",
    "#         self.class_path = os.path.join(proj_path, self.proj_name + \"_classification\")\n",
    "#         self.ts_path = os.path.join(proj_path, self.proj_name + \"_download_timeseries\")\n",
    "#         self.pt_classes = pd.read_csv(os.path.join(self.class_path,\"location_classification.csv\"))\n",
    "#         self.pt_classes = classes[['loc_id', class_colname]].dropna()\n",
    "#         # self.pt_classes['loc_id'] = self.pt_classes['loc_id'] + 10.5 # for testing index only\n",
    "#         self.classes = pd.unique(self.pt_classes[class_colname])\n",
    "#         self.labels = self.pt_classes.assign(val = 1).pivot_table(columns = class_colname, index = 'loc_id', values = 'val', fill_value= 0)\n",
    "\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         loc_id = self.labels.index[idx]\n",
    "#         self.last_loc_id = loc_id\n",
    "        \n",
    "#         # select location id\n",
    "#         s2_ts_x = s2_ts[['B8','B4','B3','B2','day']]\n",
    "#         x = torch.tensor(s2_ts_x.to_numpy())\n",
    "        \n",
    "#         # get one-hot encoding for the point as tensor\n",
    "#         y = torch.tensor(self.labels.iloc[idx].to_numpy())\n",
    "        \n",
    "#         return x, y\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return self.pt_classes.shape[0]\n",
    "\n",
    "\n",
    "# proj_path = \"/Users/gopal/Google Drive/_Research/Research projects/ML/manclassify/app_data/Thailand\"\n",
    "# # date_rangeX = pd.to_datetime(['2019-06-01','2020-05-31'])\n",
    "# s2_train = s2Dataset(proj_path = proj_path, class_colname = 'Subclass2019')\n",
    "# x = s2_train.__getitem__(10)\n",
    "# sys.getsizeof(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlnightly",
   "language": "python",
   "name": "dlnightly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
